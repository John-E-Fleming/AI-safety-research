{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8-9: nnsight Setup with Qwen\n",
    "\n",
    "**Goal:** Learn nnsight fundamentals and verify setup with a probe verification exercise\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand why we're transitioning from TransformerLens to nnsight\n",
    "2. Load and interact with Qwen2.5-7B-Instruct using nnsight\n",
    "3. Extract activations from any layer\n",
    "4. Perform basic interventions (activation patching)\n",
    "5. Verify setup works by training a sentiment probe\n",
    "\n",
    "**Timeline:** 6-7 hours\n",
    "\n",
    "**Environment:** Vast.ai with GPU (16GB+ VRAM recommended)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why nnsight Instead of TransformerLens?\n",
    "\n",
    "In Days 3-6, you used **TransformerLens** with GPT-2 to learn probing fundamentals. Now we're switching to **nnsight** for your actual CoT faithfulness research.\n",
    "\n",
    "**The key difference:**\n",
    "\n",
    "| Aspect | TransformerLens | nnsight |\n",
    "|--------|-----------------|--------|\n",
    "| **Model support** | GPT-2, GPT-Neo, limited models | ANY HuggingFace model |\n",
    "| **Qwen/Llama/DeepSeek** | NOT SUPPORTED | Fully supported |\n",
    "| **CoT reasoning models** | Cannot use | Required for this |\n",
    "| **Best use case** | Learning mech interp basics | Production research on modern models |\n",
    "\n",
    "**Bottom line:** TransformerLens doesn't support Qwen, Llama, or other modern reasoning models. For CoT faithfulness research, you MUST use nnsight (or similar tools like nnsight).\n",
    "\n",
    "GPT-2 doesn't do meaningful chain-of-thought reasoning - it's too small. You need a model like Qwen2.5-7B-Instruct that actually reasons through problems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mSetup complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this first on a fresh Vast.ai instance)\n",
    "# Uncomment and run if packages aren't installed\n",
    "import sys\n",
    "\n",
    "# For individual package installs\n",
    "!{sys.executable} -m pip install -q nnsight transformers accelerate matplotlib pandas scikit-learn\n",
    "\n",
    "# Or from requirements file\n",
    "#!{sys.executable} -m pip install -q -r requirements.txt\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU Memory: 25.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    print(\"If on Vast.ai, make sure you selected a GPU instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# nnsight - the key library for this notebook\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading Qwen with nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2.5-7B-Instruct... (this may take a few minutes on first run)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed9e898a8aa41278d7841b23cdcf347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f813be31c404da9b22aee9c98ba55fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04a6941870f49208ce2b0863b10bb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623df3df5b694f7a8e11e41b8b07b1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01ea190be094fb9abf6c8e7a52403aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Qwen2.5-7B-Instruct with nnsight\n",
    "# This will download the model on first run (~14GB)\n",
    "\n",
    "print(\"Loading Qwen2.5-7B-Instruct... (this may take a few minutes on first run)\")\n",
    "\n",
    "model = LanguageModel(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    device_map=\"auto\",  # Automatically place on GPU\n",
    "    torch_dtype=torch.float16  # Use half precision to save memory\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "==================================================\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Explore the model architecture\n",
    "# This is DIFFERENT from TransformerLens - we're working with HuggingFace structure\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: qwen2\n",
      "Hidden size (d_model): 3584\n",
      "Number of layers: 28\n",
      "Number of attention heads: 28\n",
      "Vocabulary size: 152064\n"
     ]
    }
   ],
   "source": [
    "# Key model properties for Qwen2.5-7B\n",
    "# Access the underlying HuggingFace model config\n",
    "\n",
    "config = model.config\n",
    "print(f\"Model: {config.model_type}\")\n",
    "print(f\"Hidden size (d_model): {config.hidden_size}\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Structural Differences from TransformerLens\n",
    "\n",
    "| TransformerLens (GPT-2) | nnsight (Qwen) |\n",
    "|------------------------|----------------|\n",
    "| `model.blocks[i]` | `model.model.layers[i]` |\n",
    "| `cache[\"resid_post\", layer]` | `model.model.layers[layer].output[0]` |\n",
    "| 12 layers, 768 hidden | 28 layers, 3584 hidden |\n",
    "| Automatic caching | Explicit `.save()` required |\n",
    "\n",
    "The nnsight library wraps the HuggingFace model, so we access layers through `model.model.layers[i]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Basic Tracing - Saving Activations\n",
    "\n",
    "The core concept in nnsight is the **trace context**. Inside a `with model.trace()` block:\n",
    "- Operations create \"proxy\" objects\n",
    "- Nothing executes until the context exits\n",
    "- Use `.save()` to preserve values you want to access later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5630f9d6712f4de6a7a3d3cad0771699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af27868e0f9415186bfc2604e306e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cdaf962b77469482fe0415e085306e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998c3988532049b2b0884516888a1400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51bb41db6234b12b090c32e0495cc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef925436ce648699aae895e716385e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3413ce74e10b4efbbae2b640f9e80755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f2f98517524ce3bda8642beabab875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     hidden_states = model.model.layers[\u001b[32m14\u001b[39m].output[\u001b[32m0\u001b[39m].save()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# After the context exits, hidden_states.value contains the actual tensor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHidden states shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected: [batch=1, seq_len, hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.hidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "# Simplest example: save hidden states at one layer\n",
    "\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    # Access layer 14 (middle of the 28 layers)\n",
    "    # .output[0] gets the hidden states (first element of output tuple)\n",
    "    hidden_states = model.model.layers[14].output[0].save()\n",
    "\n",
    "# After the context exits, hidden_states contains the tensor directly\n",
    "# NOTE: nnsight returns [seq_len, hidden_size] - no batch dimension\n",
    "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "print(f\"Expected: [seq_len, hidden_size={config.hidden_size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of hidden_states: <class 'torch.Tensor'>\n",
      "Hidden states shape: torch.Size([4, 3584])\n",
      "Expected: [batch=1, seq_len, hidden_size=3584]\n"
     ]
    }
   ],
   "source": [
    "# Understanding the shape:\n",
    "# - No batch dimension (nnsight squeezes it out for single inputs)\n",
    "# - Sequence length (number of tokens in \"Hello, world!\")\n",
    "# - Hidden dimension (3584 for Qwen2.5-7B)\n",
    "\n",
    "# Let's check the tokenization\n",
    "tokens = model.tokenizer.encode(\"Hello, world!\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token strings: {[model.tokenizer.decode([t]) for t in tokens]}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations at each layer:\n",
      "  Layer  0: shape = torch.Size([7, 3584])\n",
      "  Layer  7: shape = torch.Size([7, 3584])\n",
      "  Layer 14: shape = torch.Size([7, 3584])\n",
      "  Layer 21: shape = torch.Size([7, 3584])\n",
      "  Layer 27: shape = torch.Size([7, 3584])\n"
     ]
    }
   ],
   "source": [
    "# Save activations at MULTIPLE layers\n",
    "\n",
    "layers_to_extract = [0, 7, 14, 21, 27]  # Early, mid, late layers\n",
    "saved_activations = {}\n",
    "\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "    for layer in layers_to_extract:\n",
    "        saved_activations[layer] = model.model.layers[layer].output[0].save()\n",
    "\n",
    "# Check shapes - no .value needed, no batch dimension\n",
    "print(\"Activations at each layer:\")\n",
    "for layer, acts in saved_activations.items():\n",
    "    print(f\"  Layer {layer:2d}: shape = {acts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation shape: (3584,)\n",
      "Expected: (3584,)\n"
     ]
    }
   ],
   "source": [
    "# Helper function: Extract final token activation at specified layer\n",
    "# This mirrors what we did in TransformerLens\n",
    "\n",
    "def get_final_token_activation_nnsight(model, text, layer):\n",
    "    \"\"\"\n",
    "    Extract the activation of the final token at a specified layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string\n",
    "        layer: Layer number (0 to num_layers-1)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    with model.trace(text):\n",
    "        hidden = model.model.layers[layer].output[0].save()\n",
    "    \n",
    "    # No .value needed, shape is [seq_len, hidden_size] (no batch dim)\n",
    "    # Need .detach() before .numpy() since tensor has grad\n",
    "    final_act = hidden[-1, :].detach().cpu().numpy()\n",
    "    return final_act\n",
    "\n",
    "# Test it\n",
    "test_act = get_final_token_activation_nnsight(model, \"Test sentence.\", layer=14)\n",
    "print(f\"Activation shape: {test_act.shape}\")\n",
    "print(f\"Expected: ({config.hidden_size},)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: TransformerLens vs nnsight Activation Extraction\n",
    "\n",
    "**TransformerLens:**\n",
    "```python\n",
    "_, cache = model.run_with_cache(text)\n",
    "activation = cache[\"resid_post\", layer][0, -1, :].cpu().numpy()\n",
    "```\n",
    "\n",
    "**nnsight (current API):**\n",
    "```python\n",
    "with model.trace(text):\n",
    "    hidden = model.model.layers[layer].output[0].save()\n",
    "activation = hidden[-1, :].detach().cpu().numpy()  # No .value, no batch dim\n",
    "```\n",
    "\n",
    "Key differences:\n",
    "1. nnsight requires explicit `.save()` - values aren't automatically cached\n",
    "2. nnsight uses deferred execution - operations run when context exits\n",
    "3. nnsight uses HuggingFace model structure (`model.model.layers`)\n",
    "4. nnsight returns tensors directly (no `.value` accessor needed)\n",
    "5. Shape is `[seq_len, hidden_size]` - no batch dimension\n",
    "6. Need `.detach()` before `.numpy()` since tensors have gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3.5: Accessing Attention and MLP Components\n",
    "\n",
    "In Day 5-6, you learned that different components (attention vs MLP) serve different computational roles:\n",
    "- **Attention:** Routes information between positions (\"where should I look?\")\n",
    "- **MLP:** Transforms information within position (\"what should I compute?\")\n",
    "\n",
    "Here's how to access these components in nnsight (compared to TransformerLens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of layer 0:\n",
      "==================================================\n",
      "  self_attn: Qwen2Attention\n",
      "  mlp: Qwen2MLP\n",
      "  input_layernorm: Qwen2RMSNorm\n",
      "  post_attention_layernorm: Qwen2RMSNorm\n"
     ]
    }
   ],
   "source": [
    "# First, let's explore the structure of a single layer in Qwen\n",
    "# This shows us what submodules we can access\n",
    "\n",
    "print(\"Structure of layer 0:\")\n",
    "print(\"=\" * 50)\n",
    "for name, module in model.model.layers[0].named_children():\n",
    "    print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP output shape: torch.Size([1, 7, 3584])\n",
      "Layer output shape: torch.Size([7, 3584])\n",
      "\n",
      "Both should have hidden_size=3584 as last dimension\n"
     ]
    }
   ],
   "source": [
    "# Access MLP output at a specific layer\n",
    "# The MLP in Qwen is called \"mlp\" (in GPT-2/TransformerLens it was also \"mlp\")\n",
    "#\n",
    "# IMPORTANT: In nnsight, you should access different component types in SEPARATE traces\n",
    "# Mixing layer output and submodule output in one trace can cause OutOfOrderError\n",
    "\n",
    "# Get MLP output in its own trace\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "   mlp_output = model.model.layers[14].mlp.output.save()\n",
    "\n",
    "# Get layer output in a separate trace\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "   layer_output = model.model.layers[14].output[0].save()\n",
    "\n",
    "print(f\"MLP output shape: {mlp_output.shape}\")\n",
    "print(f\"Layer output shape: {layer_output.shape}\")\n",
    "print(f\"\\nBoth should have hidden_size={config.hidden_size} as last dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([1, 7, 3584])\n",
      "Expected: [1, seq_len, hidden_size=3584]\n",
      "\n",
      "Note: self_attn.output includes batch dim, but layers[].output[0] does not!\n"
     ]
    }
   ],
   "source": [
    "# Access Attention output at a specific layer\n",
    "# In Qwen, the self-attention module is called \"self_attn\"\n",
    "\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "    # Get the attention output (after attention computation, before adding to residual)\n",
    "    # Note: attention output is typically a tuple, we want the first element\n",
    "    attn_output = model.model.layers[14].self_attn.output[0].save()\n",
    "\n",
    "# No .value needed\n",
    "# NOTE: Attention output DOES have batch dimension: [batch, seq_len, hidden_size]\n",
    "# This is different from layer output which has no batch dim!\n",
    "print(f\"Attention output shape: {attn_output.shape}\")\n",
    "print(f\"Expected: [1, seq_len, hidden_size={config.hidden_size}]\")\n",
    "print(f\"\\nNote: self_attn.output includes batch dim, but layers[].output[0] does not!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation: sdpa\n",
      "\n",
      "With 'sdpa' attention:\n",
      "  ✓ Can access attention OUTPUT (what we need for probing)\n",
      "  ✗ Cannot access attention PATTERNS (weights after softmax)\n",
      "\n",
      "This is fine for faithfulness probing - we probe the outputs, not the weights!\n"
     ]
    }
   ],
   "source": [
    "# Access Attention PATTERNS (the attention weights after softmax)\n",
    "# \n",
    "# NOTE: This requires \"eager\" attention implementation, but modern models\n",
    "# default to SDPA (Scaled Dot Product Attention) for speed.\n",
    "# \n",
    "# For probing, we usually care about attention OUTPUT (the result), not\n",
    "# the attention PATTERNS (the weights). We already have the output!\n",
    "#\n",
    "# If you need patterns for visualization/analysis, you'd need to reload\n",
    "# the model with: attn_implementation=\"eager\" (but this is slower)\n",
    "\n",
    "# Let's verify what attention implementation we're using:\n",
    "print(f\"Attention implementation: {model.config._attn_implementation}\")\n",
    "print(f\"\\nWith '{model.config._attn_implementation}' attention:\")\n",
    "print(\"  ✓ Can access attention OUTPUT (what we need for probing)\")\n",
    "print(\"  ✗ Cannot access attention PATTERNS (weights after softmax)\")\n",
    "print(\"\\nThis is fine for faithfulness probing - we probe the outputs, not the weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping attention pattern extraction (using fast SDPA attention)\n",
      "\n",
      "For faithfulness probing, we need:\n",
      "  ✓ Layer outputs (residual stream) - AVAILABLE\n",
      "  ✓ MLP outputs - AVAILABLE\n",
      "  ✓ Attention outputs - AVAILABLE\n",
      "  - Attention patterns (weights) - Not needed for probing\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: If you really need attention patterns, you can reload with eager attention\n",
    "# This is SLOWER but allows accessing the attention weights\n",
    "#\n",
    "# Uncomment below ONLY if you need attention pattern visualization:\n",
    "#\n",
    "# model_eager = LanguageModel(\n",
    "#     \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     attn_implementation=\"eager\"  # Enables output_attentions\n",
    "# )\n",
    "#\n",
    "# model_eager.config.output_attentions = True\n",
    "# with model_eager.trace(\"The capital of France is Paris.\"):\n",
    "#     attn_with_weights = model_eager.model.layers[14].self_attn.output.save()\n",
    "\n",
    "# For this notebook, we'll skip attention patterns since:\n",
    "# 1. We're focused on probing activation OUTPUTS\n",
    "# 2. SDPA is much faster for training/inference\n",
    "# 3. Attention patterns are mainly useful for interpretability visualization\n",
    "\n",
    "print(\"Skipping attention pattern extraction (using fast SDPA attention)\")\n",
    "print(\"\\nFor faithfulness probing, we need:\")\n",
    "print(\"  ✓ Layer outputs (residual stream) - AVAILABLE\")  \n",
    "print(\"  ✓ MLP outputs - AVAILABLE\")\n",
    "print(\"  ✓ Attention outputs - AVAILABLE\")\n",
    "print(\"  - Attention patterns (weights) - Not needed for probing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing component extraction functions:\n",
      "==================================================\n",
      "MLP output shape: (3, 3584)\n",
      "Attention output shape: (3584,)\n",
      "Attention patterns: Not available (model uses SDPA - this is expected)\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for extracting different components\n",
    "# Updated for current nnsight API:\n",
    "# - No .value accessor needed\n",
    "# - Need .detach() before .numpy()\n",
    "# - IMPORTANT: Different modules have different batch dimension behavior:\n",
    "#   * layers[].output[0]: NO batch dim - shape [seq_len, hidden_size]\n",
    "#   * mlp.output: NO batch dim - shape [seq_len, hidden_size]\n",
    "#   * self_attn.output[0]: HAS batch dim - shape [batch, seq_len, hidden_size]\n",
    "\n",
    "def get_mlp_output(model, text, layer, position='last'):\n",
    "    \"\"\"\n",
    "    Extract MLP output at a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string\n",
    "        layer: Layer number\n",
    "        position: 'last', 'first', 'mean', or int\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    with model.trace(text):\n",
    "        mlp_out = model.model.layers[layer].mlp.output.save()\n",
    "    \n",
    "    # MLP output has NO batch dim - shape is [seq_len, hidden_size]\n",
    "    acts = mlp_out\n",
    "    \n",
    "    if position == 'last':\n",
    "        return acts[-1, :].detach().cpu().numpy()\n",
    "    elif position == 'first':\n",
    "        return acts[0, :].detach().cpu().numpy()\n",
    "    elif position == 'mean':\n",
    "        return acts.mean(dim=0).detach().cpu().numpy()\n",
    "    elif isinstance(position, int):\n",
    "        return acts[position, :].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def get_attention_output(model, text, layer, position='last'):\n",
    "    \"\"\"\n",
    "    Extract attention output at a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string  \n",
    "        layer: Layer number\n",
    "        position: 'last', 'first', 'mean', or int\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    with model.trace(text):\n",
    "        attn_out = model.model.layers[layer].self_attn.output[0].save()\n",
    "    \n",
    "    # Attention output HAS batch dim - shape is [batch, seq_len, hidden_size]\n",
    "    # Index with [0] to remove batch dimension first\n",
    "    acts = attn_out[0]  # Now [seq_len, hidden_size]\n",
    "    \n",
    "    if position == 'last':\n",
    "        return acts[-1, :].detach().cpu().numpy()\n",
    "    elif position == 'first':\n",
    "        return acts[0, :].detach().cpu().numpy()\n",
    "    elif position == 'mean':\n",
    "        return acts.mean(dim=0).detach().cpu().numpy()\n",
    "    elif isinstance(position, int):\n",
    "        return acts[position, :].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def get_attention_patterns(model, text, layer):\n",
    "    \"\"\"\n",
    "    Extract attention patterns (weights after softmax) at a specific layer.\n",
    "    \n",
    "    NOTE: This only works if the model uses \"eager\" attention implementation.\n",
    "    With SDPA (default for speed), attention patterns are not available.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string\n",
    "        layer: Layer number\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (num_heads, seq_len, seq_len) or None\n",
    "    \"\"\"\n",
    "    # Check if we can access attention patterns\n",
    "    if hasattr(model.config, '_attn_implementation'):\n",
    "        if model.config._attn_implementation != \"eager\":\n",
    "            # Can't get patterns with SDPA/flash attention\n",
    "            return None\n",
    "    \n",
    "    # Try to enable attention output\n",
    "    try:\n",
    "        original = model.config.output_attentions\n",
    "        model.config.output_attentions = True\n",
    "        \n",
    "        with model.trace(text):\n",
    "            attn_out = model.model.layers[layer].self_attn.output.save()\n",
    "        \n",
    "        model.config.output_attentions = original\n",
    "        \n",
    "        # Attention weights are typically the second element of the tuple\n",
    "        if isinstance(attn_out, tuple) and len(attn_out) > 1:\n",
    "            patterns = attn_out[1]\n",
    "            if patterns is not None:\n",
    "                # Remove batch dim if present\n",
    "                if len(patterns.shape) == 4:\n",
    "                    patterns = patterns[0]\n",
    "                return patterns.detach().cpu().numpy()\n",
    "    except ValueError:\n",
    "        # SDPA doesn't support output_attentions\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Test the helper functions\n",
    "print(\"Testing component extraction functions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_mlp = get_mlp_output(model, \"Test sentence.\", layer=14, position='last')\n",
    "print(f\"MLP output shape: {test_mlp.shape}\")\n",
    "\n",
    "test_attn = get_attention_output(model, \"Test sentence.\", layer=14, position='last')\n",
    "print(f\"Attention output shape: {test_attn.shape}\")\n",
    "\n",
    "test_patterns = get_attention_patterns(model, \"Test sentence.\", layer=14)\n",
    "if test_patterns is not None:\n",
    "    print(f\"Attention patterns shape: {test_patterns.shape}\")\n",
    "else:\n",
    "    print(\"Attention patterns: Not available (model uses SDPA - this is expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Access Comparison: TransformerLens vs nnsight\n",
    "\n",
    "| Component | TransformerLens | nnsight (Qwen) |\n",
    "|-----------|-----------------|----------------|\n",
    "| **Residual stream (layer output)** | `cache[\"resid_post\", layer]` | `model.model.layers[layer].output[0]` |\n",
    "| **MLP output** | `cache[\"blocks.{layer}.hook_mlp_out\"]` | `model.model.layers[layer].mlp.output` |\n",
    "| **Attention output** | `cache[\"attn_out\", layer]` | `model.model.layers[layer].self_attn.output[0]` |\n",
    "| **Attention patterns** | `cache[\"blocks.{layer}.attn.hook_pattern\"]` | Enable `output_attentions=True`, then `self_attn.output[1]` |\n",
    "| **Individual head outputs** | `cache[\"blocks.{layer}.attn.hook_z\"]` | Requires accessing internal attention computation |\n",
    "\n",
    "**Key insight from Day 5-6:** You found that middle-layer attention heads outperformed residual stream probes for sentiment. With these extraction functions, you can    \n",
    "test if the same pattern holds for Qwen and for faithfulness detection.\n",
    "\n",
    "**Important nnsight limitations:**\n",
    "1. **Shape differences:** `layers[].output[0]` and `mlp.output` have NO batch dimension `[seq_len, hidden_size]`, but `self_attn.output[0]` HAS batch dimension\n",
    "`[batch, seq_len, hidden_size]`\n",
    "2. **Separate traces required:** You must extract different component types (layer output vs mlp vs attention) in SEPARATE `model.trace()` calls. Mixing them in one     \n",
    "trace causes `OutOfOrderError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component activations at layer 14, final token:\n",
      "  Residual stream: norm = 72.38\n",
      "  MLP output:      norm = 57.97\n",
      "  Attention output: norm = 15.50\n"
     ]
    }
   ],
   "source": [
    "# Quick comparison: Residual vs MLP vs Attention at the same layer\n",
    "# This mirrors the analysis you did in Day 5-6\n",
    "#\n",
    "# IMPORTANT: Extract each component in a SEPARATE trace to avoid OutOfOrderError\n",
    "\n",
    "test_text = \"I love this movie!\"\n",
    "layer = 14\n",
    "\n",
    "# Extract each component in its own trace\n",
    "with model.trace(test_text):\n",
    "   residual = model.model.layers[layer].output[0].save()\n",
    "\n",
    "with model.trace(test_text):\n",
    "   mlp = model.model.layers[layer].mlp.output.save()\n",
    "\n",
    "with model.trace(test_text):\n",
    "   attn = model.model.layers[layer].self_attn.output[0].save()\n",
    "\n",
    "# Get final token activations\n",
    "# Note the different indexing due to batch dimension differences:\n",
    "# - residual & mlp: [seq_len, hidden_size] - no batch dim\n",
    "# - attn: [batch, seq_len, hidden_size] - has batch dim\n",
    "residual_act = residual[-1, :].detach().cpu().numpy()\n",
    "mlp_act = mlp[-1, :].detach().cpu().numpy()\n",
    "attn_act = attn[0, -1, :].detach().cpu().numpy()  # [0] removes batch dim\n",
    "\n",
    "print(f\"Component activations at layer {layer}, final token:\")\n",
    "print(f\"  Residual stream: norm = {np.linalg.norm(residual_act):.2f}\")\n",
    "print(f\"  MLP output:      norm = {np.linalg.norm(mlp_act):.2f}\")\n",
    "print(f\"  Attention output: norm = {np.linalg.norm(attn_act):.2f}\")\n",
    "\n",
    "# Note: residual ≈ previous_residual + attention + mlp\n",
    "# So residual has accumulated information from both components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Generation with Activation Access\n",
    "\n",
    "For CoT faithfulness research, you need to:\n",
    "1. Generate reasoning (multi-token output)\n",
    "2. Access activations during that generation\n",
    "\n",
    "nnsight uses `.generate()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "==================================================\n",
      "What is 17 * 23? Let me think step by step. To solve 17 * 23, I'll use the standard multiplication algorithm:\n",
      "\n",
      "Step 1: Multiply 23 by 7 (the ones digit of 17)\n",
      "23 * 7 = 161\n",
      "\n",
      "Step 2: Multiply 23 by 1 (the tens digit of 17), and remember to shift one place to the left\n",
      "23 * 10 = 230\n",
      "\n",
      "Step 3: Add the results from steps 1 and 2\n",
      "161 + 230 = 391\n",
      "\n",
      "Therefore, 17 * 23 = 391. \n",
      "\n",
      "You can verify this by using a calculator or another method of multiplication.\n",
      "==================================================\n",
      "\n",
      "Activations shape: torch.Size([167, 3584])\n",
      "This represents activations for the full generated sequence.\n"
     ]
    }
   ],
   "source": [
    "# Generate text with activation access\n",
    "# \n",
    "# NOTE: The nnsight generation API has changed. We need to use a different approach.\n",
    "# For now, let's generate text and extract activations separately.\n",
    "\n",
    "prompt = \"What is 17 * 23? Let me think step by step.\"\n",
    "\n",
    "# Tokenize the prompt with attention mask\n",
    "inputs = model.tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "# Generate using the model directly (greedy decoding)\n",
    "with torch.no_grad():\n",
    "   output_ids = model.generate(\n",
    "       input_ids,\n",
    "       attention_mask=attention_mask,\n",
    "       max_new_tokens=150,\n",
    "       do_sample=False,  # Greedy decoding\n",
    "       pad_token_id=model.tokenizer.eos_token_id  # Suppress padding warning\n",
    "   )\n",
    "\n",
    "# Decode the output\n",
    "output_text = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(\"=\" * 50)\n",
    "print(output_text)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Now extract activations for the full sequence (prompt + generated)\n",
    "with model.trace(output_text):\n",
    "   hidden_during_gen = model.model.layers[14].output[0].save()\n",
    "\n",
    "print(f\"\\nActivations shape: {hidden_during_gen.shape}\")\n",
    "print(f\"This represents activations for the full generated sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 17\n",
      "Total tokens (prompt + generated): 167\n",
      "Activation sequence length: 167\n",
      "Generated tokens: 150\n"
     ]
    }
   ],
   "source": [
    "# Understanding the activation shape during generation:\n",
    "# Shape is [total_sequence_length, hidden_size] (no batch dim)\n",
    "# total_sequence_length = prompt_tokens + generated_tokens\n",
    "\n",
    "prompt_tokens = model.tokenizer.encode(prompt)\n",
    "total_tokens = model.tokenizer.encode(output_text)\n",
    "print(f\"Prompt tokens: {len(prompt_tokens)}\")\n",
    "print(f\"Total tokens (prompt + generated): {len(total_tokens)}\")\n",
    "print(f\"Activation sequence length: {hidden_during_gen.shape[0]}\")\n",
    "print(f\"Generated tokens: {len(total_tokens) - len(prompt_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-Activation mapping (first 20 tokens):\n",
      "------------------------------------------------------------\n",
      "Pos   0: 'What           ' | Activation norm: 15120.00\n",
      "Pos   1: ' is            ' | Activation norm: 89.25\n",
      "Pos   2: '               ' | Activation norm: 88.12\n",
      "Pos   3: '1              ' | Activation norm: 79.38\n",
      "Pos   4: '7              ' | Activation norm: 77.88\n",
      "Pos   5: ' *             ' | Activation norm: 66.94\n",
      "Pos   6: '               ' | Activation norm: 72.44\n",
      "Pos   7: '2              ' | Activation norm: 69.81\n",
      "Pos   8: '3              ' | Activation norm: 69.56\n",
      "Pos   9: '?              ' | Activation norm: 70.38\n",
      "Pos  10: ' Let           ' | Activation norm: 62.34\n",
      "Pos  11: ' me            ' | Activation norm: 64.50\n",
      "Pos  12: ' think         ' | Activation norm: 66.31\n",
      "Pos  13: ' step          ' | Activation norm: 65.00\n",
      "Pos  14: ' by            ' | Activation norm: 68.38\n",
      "Pos  15: ' step          ' | Activation norm: 64.44\n",
      "Pos  16: '.              ' | Activation norm: 56.22\n",
      "Pos  17: ' To            ' | Activation norm: 60.47\n",
      "Pos  18: ' solve         ' | Activation norm: 70.69\n",
      "Pos  19: '               ' | Activation norm: 64.81\n"
     ]
    }
   ],
   "source": [
    "# Map tokens to their activations\n",
    "# This is important for position analysis in faithfulness research\n",
    "\n",
    "all_token_ids = model.tokenizer.encode(output_text)\n",
    "all_tokens = [model.tokenizer.decode([t]) for t in all_token_ids]\n",
    "\n",
    "print(\"Token-Activation mapping (first 20 tokens):\")\n",
    "print(\"-\" * 60)\n",
    "for i, (tok_id, tok_str) in enumerate(zip(all_token_ids[:20], all_tokens[:20])):\n",
    "   # No batch dim, so index directly with position\n",
    "   act_norm = hidden_during_gen[i, :].norm().item()\n",
    "   print(f\"Pos {i:3d}: '{tok_str:15s}' | Activation norm: {act_norm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Intervention Basics\n",
    "\n",
    "Beyond reading activations, nnsight lets you **modify** them during forward pass. This is activation patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: The capital of France is Paris. Which of the\n",
      "\n",
      "--- Intervention Demo (Forward Pass) ---\n",
      "Clean next token prediction: ''\n",
      "Intervened next token prediction: ''\n",
      "Intervention changed prediction: False\n"
     ]
    }
   ],
   "source": [
    "# Simple intervention: zero out the last token's activation at layer 10\n",
    "#\n",
    "# NOTE: With the current nnsight API, interventions during generation are complex.\n",
    "# Instead, we'll demonstrate intervention during a forward pass (trace).\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = model.tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "# Baseline generation (no intervention)\n",
    "with torch.no_grad():\n",
    "   baseline_ids = model.generate(\n",
    "       input_ids,\n",
    "       attention_mask=attention_mask,\n",
    "       max_new_tokens=5,\n",
    "       do_sample=False,\n",
    "       pad_token_id=model.tokenizer.eos_token_id\n",
    "   )\n",
    "\n",
    "baseline_output = model.tokenizer.decode(baseline_ids[0], skip_special_tokens=True)\n",
    "print(f\"Baseline: {baseline_output}\")\n",
    "\n",
    "# For intervention, we can demonstrate with a trace (forward pass)\n",
    "# This shows that we CAN modify activations, even if generation intervention is complex\n",
    "print(\"\\n--- Intervention Demo (Forward Pass) ---\")\n",
    "\n",
    "# Get logits WITHOUT intervention\n",
    "with model.trace(prompt):\n",
    "   clean_logits = model.lm_head.output.save()\n",
    "\n",
    "# Get logits WITH intervention (zero out layer 10 last position)\n",
    "with model.trace(prompt):\n",
    "   # Zero out the last position at layer 10\n",
    "   model.model.layers[10].output[0][-1, :] = 0\n",
    "   intervened_logits = model.lm_head.output.save()\n",
    "\n",
    "# Compare the top predicted tokens\n",
    "clean_next_token = clean_logits[-1, :].argmax().item()\n",
    "intervened_next_token = intervened_logits[-1, :].argmax().item()\n",
    "\n",
    "print(f\"Clean next token prediction: '{model.tokenizer.decode([clean_next_token])}'\")\n",
    "print(f\"Intervened next token prediction: '{model.tokenizer.decode([intervened_next_token])}'\")\n",
    "print(f\"Intervention changed prediction: {clean_next_token != intervened_next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source activation shape: torch.Size([5, 3584])\n",
      "Clean logits shape: torch.Size([1, 5, 152064])\n",
      "Clean last logits shape: torch.Size([152064])\n",
      "\n",
      "Target prompt: 'The capital of France is'\n",
      "Clean next token: ' Paris'\n",
      "Patched next token: ' Berlin'\n",
      "\n",
      "Patching changed prediction: True\n",
      "\n",
      "--- Top-5 Predictions Comparison ---\n",
      "Top5 vals shape: torch.Size([5])\n",
      "Clean (France prompt):\n",
      "  1. ' Paris' (49.96%)\n",
      "  2. ' __' (7.37%)\n",
      "  3. ' a' (5.74%)\n",
      "  4. ' ____' (4.83%)\n",
      "  5. ' ______' (4.10%)\n",
      "\n",
      "Patched (with Germany activations):\n",
      "  1. ' Berlin' (31.35%)\n",
      "  2. ' __' (11.72%)\n",
      "  3. ' a' (10.92%)\n",
      "  4. ' ______' (6.68%)\n",
      "  5. ' ____' (5.28%)\n",
      "\n",
      "--- Specific Token Probability Comparison ---\n",
      "P(' Paris') - Clean: 49.9630%, Patched: 0.0066%\n",
      "P(' Berlin') - Clean: 0.0257%, Patched: 31.3535%\n"
     ]
    }
   ],
   "source": [
    "# Cross-prompt intervention (activation patching)\n",
    "# Extract activation from one prompt, inject into another\n",
    "#\n",
    "# This demonstrates the core technique used in mechanistic interpretability:\n",
    "# \"What happens if we replace the representation of X with the representation of Y?\"\n",
    "\n",
    "source_prompt = \"The capital of Germany is\"\n",
    "target_prompt = \"The capital of France is\"\n",
    "\n",
    "# Step 1: Extract activation from source prompt\n",
    "with model.trace(source_prompt):\n",
    "   source_activation = model.model.layers[14].output[0].save()\n",
    "\n",
    "print(f\"Source activation shape: {source_activation.shape}\")\n",
    "\n",
    "# Step 2: Get clean prediction for target\n",
    "with model.trace(target_prompt):\n",
    "   clean_logits = model.lm_head.output.save()\n",
    "\n",
    "print(f\"Clean logits shape: {clean_logits.shape}\")\n",
    "\n",
    "# Get logits for the last token position - handle different possible shapes\n",
    "if len(clean_logits.shape) == 3:\n",
    "   # Shape is [batch, seq_len, vocab_size]\n",
    "   clean_last_logits = clean_logits[0, -1, :]\n",
    "elif len(clean_logits.shape) == 2:\n",
    "   # Shape is [seq_len, vocab_size]\n",
    "   clean_last_logits = clean_logits[-1, :]\n",
    "else:\n",
    "   clean_last_logits = clean_logits\n",
    "\n",
    "print(f\"Clean last logits shape: {clean_last_logits.shape}\")\n",
    "\n",
    "clean_next_token = clean_last_logits.argmax().item()\n",
    "print(f\"\\nTarget prompt: '{target_prompt}'\")\n",
    "print(f\"Clean next token: '{model.tokenizer.decode([clean_next_token])}'\")\n",
    "\n",
    "# Step 3: Patch source activation into target and get new prediction\n",
    "with model.trace(target_prompt):\n",
    "   # Replace layer 14 activation with source activation\n",
    "   model.model.layers[14].output[0][:, :] = source_activation\n",
    "   patched_logits = model.lm_head.output.save()\n",
    "\n",
    "# Handle different possible shapes\n",
    "if len(patched_logits.shape) == 3:\n",
    "   patched_last_logits = patched_logits[0, -1, :]\n",
    "elif len(patched_logits.shape) == 2:\n",
    "   patched_last_logits = patched_logits[-1, :]\n",
    "else:\n",
    "   patched_last_logits = patched_logits\n",
    "\n",
    "patched_next_token = patched_last_logits.argmax().item()\n",
    "print(f\"Patched next token: '{model.tokenizer.decode([patched_next_token])}'\")\n",
    "print(f\"\\nPatching changed prediction: {clean_next_token != patched_next_token}\")\n",
    "\n",
    "# Show top-5 predictions for both\n",
    "print(\"\\n--- Top-5 Predictions Comparison ---\")\n",
    "clean_probs = torch.softmax(clean_last_logits.float(), dim=-1)\n",
    "patched_probs = torch.softmax(patched_last_logits.float(), dim=-1)\n",
    "\n",
    "# Get top 5 - flatten to ensure 1D\n",
    "clean_probs_flat = clean_probs.flatten()\n",
    "patched_probs_flat = patched_probs.flatten()\n",
    "\n",
    "clean_top5_vals, clean_top5_idxs = clean_probs_flat.topk(5)\n",
    "patched_top5_vals, patched_top5_idxs = patched_probs_flat.topk(5)\n",
    "\n",
    "print(f\"Top5 vals shape: {clean_top5_vals.shape}\")\n",
    "\n",
    "print(\"Clean (France prompt):\")\n",
    "for i in range(5):\n",
    "   prob = clean_top5_vals[i].detach().cpu().item()\n",
    "   idx = clean_top5_idxs[i].detach().cpu().item()\n",
    "   print(f\"  {i+1}. '{model.tokenizer.decode([idx])}' ({prob:.2%})\")\n",
    "\n",
    "print(\"\\nPatched (with Germany activations):\")\n",
    "for i in range(5):\n",
    "   prob = patched_top5_vals[i].detach().cpu().item()\n",
    "   idx = patched_top5_idxs[i].detach().cpu().item()\n",
    "   print(f\"  {i+1}. '{model.tokenizer.decode([idx])}' ({prob:.2%})\")\n",
    "\n",
    "# Note: The immediate next token might just be a space or punctuation.\n",
    "# The real test is whether \"Paris\" vs \"Berlin\" probability shifts.\n",
    "print(\"\\n--- Specific Token Probability Comparison ---\")\n",
    "paris_id = model.tokenizer.encode(\" Paris\", add_special_tokens=False)[0]\n",
    "berlin_id = model.tokenizer.encode(\" Berlin\", add_special_tokens=False)[0]\n",
    "\n",
    "print(f\"P(' Paris') - Clean: {clean_probs_flat[paris_id].item():.4%}, Patched: {patched_probs_flat[paris_id].item():.4%}\")\n",
    "print(f\"P(' Berlin') - Clean: {clean_probs_flat[berlin_id].item():.4%}, Patched: {patched_probs_flat[berlin_id].item():.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: TransformerLens vs nnsight Interventions\n",
    "\n",
    "**TransformerLens (hook-based):**\n",
    "```python\n",
    "def hook_fn(activation, hook):\n",
    "   activation[:, -1, :] = 0\n",
    "   return activation\n",
    "\n",
    "model.run_with_hooks(prompt, fwd_hooks=[(\"blocks.10.hook_resid_post\", hook_fn)])\n",
    "```\n",
    "\n",
    "**nnsight (assignment-based):**\n",
    "```python\n",
    "with model.trace(prompt):\n",
    "   model.model.layers[10].output[0][-1, :] = 0  # Note: no batch dim\n",
    "   logits = model.lm_head.output.save()  # Get the resulting logits\n",
    "```\n",
    "\n",
    "nnsight's approach is more intuitive - you just assign to the values you want to change within the trace context.\n",
    "\n",
    "**Key insight:** Activation patching lets us ask causal questions like:\n",
    "- \"Does the model's prediction change if we replace France's representation with Germany's?\"\n",
    "- \"Which layer contains the 'country → capital' mapping?\"\n",
    "\n",
    "This is the foundation for understanding how CoT reasoning is represented internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Building Probe-Compatible Extraction Class\n",
    "\n",
    "Let's build a reusable class that mirrors the ProbeToolkit from TransformerLens days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor ready for model with 28 layers, 3584 hidden dim\n"
     ]
    }
   ],
   "source": [
    "class NNsightActivationExtractor:\n",
    "    \"\"\"\n",
    "    Reusable toolkit for extracting activations from nnsight models.\n",
    "    Mirrors the ProbeToolkit from TransformerLens exercises.\n",
    "    \n",
    "    Updated for current nnsight API:\n",
    "    - No .value accessor needed (tensors returned directly)\n",
    "    - No batch dimension in layer output (shape is [seq_len, hidden_size])\n",
    "    - Need .detach() before .numpy() since tensors have grad\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.config = model.config\n",
    "        self.num_layers = model.config.num_hidden_layers\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "    \n",
    "    def get_layer_activation(self, text, layer, position='last'):\n",
    "        \"\"\"\n",
    "        Extract activation at a specific layer and position.\n",
    "        \n",
    "        Args:\n",
    "            text: Input string\n",
    "            layer: Layer number (0 to num_layers-1)\n",
    "            position: 'last', 'first', 'mean', or int for specific position\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (hidden_size,)\n",
    "        \"\"\"\n",
    "        with self.model.trace(text):\n",
    "            hidden = self.model.model.layers[layer].output[0].save()\n",
    "        \n",
    "        # No .value, no batch dim - shape is [seq_len, hidden_size]\n",
    "        # Need .detach() before .numpy() since tensor has grad\n",
    "        acts = hidden\n",
    "        \n",
    "        if position == 'last':\n",
    "            return acts[-1, :].detach().cpu().numpy()\n",
    "        elif position == 'first':\n",
    "            return acts[0, :].detach().cpu().numpy()\n",
    "        elif position == 'mean':\n",
    "            return acts.mean(dim=0).detach().cpu().numpy()\n",
    "        elif isinstance(position, int):\n",
    "            return acts[position, :].detach().cpu().numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown position: {position}\")\n",
    "    \n",
    "    def get_batch_activations(self, texts, layer, position='last'):\n",
    "        \"\"\"\n",
    "        Extract activations for multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input strings\n",
    "            layer: Layer number\n",
    "            position: Position strategy\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (n_texts, hidden_size)\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        for text in texts:\n",
    "            act = self.get_layer_activation(text, layer, position)\n",
    "            activations.append(act)\n",
    "        return np.array(activations)\n",
    "    \n",
    "    def compare_layers(self, texts_pos, texts_neg, layers, position='last'):\n",
    "        \"\"\"\n",
    "        Compare probe performance across multiple layers.\n",
    "        \n",
    "        Args:\n",
    "            texts_pos: List of positive class texts\n",
    "            texts_neg: List of negative class texts\n",
    "            layers: List of layer numbers to test\n",
    "            position: Position strategy\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with layer, train_acc, test_acc\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            print(f\"Testing layer {layer}...\")\n",
    "            \n",
    "            # Extract activations\n",
    "            X_pos = self.get_batch_activations(texts_pos, layer, position)\n",
    "            X_neg = self.get_batch_activations(texts_neg, layer, position)\n",
    "            \n",
    "            # Combine into dataset\n",
    "            X = np.vstack([X_pos, X_neg])\n",
    "            y = np.array([1] * len(texts_pos) + [0] * len(texts_neg))\n",
    "            \n",
    "            # Train/test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.3, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Train probe\n",
    "            probe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            probe.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = probe.score(X_train, y_train)\n",
    "            test_acc = probe.score(X_test, y_test)\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'gap': train_acc - test_acc\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Create extractor instance\n",
    "extractor = NNsightActivationExtractor(model)\n",
    "print(f\"Extractor ready for model with {extractor.num_layers} layers, {extractor.hidden_size} hidden dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single activation shape: (3584,)\n",
      "Batch activation shape: (3, 3584)\n"
     ]
    }
   ],
   "source": [
    "# Test the extractor\n",
    "test_act = extractor.get_layer_activation(\"Test sentence.\", layer=14, position='last')\n",
    "print(f\"Single activation shape: {test_act.shape}\")\n",
    "\n",
    "test_batch = extractor.get_batch_activations(\n",
    "    [\"First test.\", \"Second test.\", \"Third test.\"],\n",
    "    layer=14,\n",
    "    position='last'\n",
    ")\n",
    "print(f\"Batch activation shape: {test_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Full Probe Verification\n",
    "\n",
    "Let's verify our setup by training the same sentiment probe from Day 3-4 on Qwen activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 23 positive, 23 negative\n"
     ]
    }
   ],
   "source": [
    "# Same sentiment dataset from Day 3-4\n",
    "positive_sentences = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is amazing and wonderful!\",\n",
    "    \"Great job, fantastic work!\",\n",
    "    \"I absolutely loved every minute of that film.\",\n",
    "    \"This is the best coffee I've ever tasted.\",\n",
    "    \"She was so kind and helpful throughout the process.\",\n",
    "    \"What a beautiful day to be outside.\",\n",
    "    \"I'm thrilled with how the project turned out.\",\n",
    "    \"The team did an outstanding job on this.\",\n",
    "    \"I can't wait to visit again next year.\",\n",
    "    \"This restaurant exceeded all my expectations.\",\n",
    "    \"He's such a talented and generous person.\",\n",
    "    \"I feel incredibly grateful for this opportunity.\",\n",
    "    \"The service here is always fantastic.\",\n",
    "    \"That was the most fun I've had in ages.\",\n",
    "    \"I'm so proud of what we accomplished together.\",\n",
    "    \"This book changed my perspective completely.\",\n",
    "    \"The sunset tonight was absolutely stunning.\",\n",
    "    \"I've never felt more welcomed anywhere.\",\n",
    "    \"Everything about this experience was delightful.\",\n",
    "    \"She gave the most inspiring speech I've ever heard.\",\n",
    "    \"I'm genuinely excited about what's next.\",\n",
    "    \"This made my whole week better.\",\n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"I hate this movie.\",\n",
    "    \"This is terrible and awful.\",\n",
    "    \"Poor job, disappointing work.\",\n",
    "    \"I was deeply disappointed by the outcome.\",\n",
    "    \"The food was cold and tasteless.\",\n",
    "    \"This has been the worst customer service experience.\",\n",
    "    \"I regret wasting my time on this.\",\n",
    "    \"The whole event was a complete disaster.\",\n",
    "    \"I'm frustrated with how poorly this was handled.\",\n",
    "    \"Nothing about this met my expectations.\",\n",
    "    \"The quality has really gone downhill.\",\n",
    "    \"I felt completely ignored the entire time.\",\n",
    "    \"This product broke after just one use.\",\n",
    "    \"What a miserable waste of money.\",\n",
    "    \"I've never been so let down by a company.\",\n",
    "    \"The atmosphere was unpleasant and unwelcoming.\",\n",
    "    \"I'm annoyed that nobody bothered to help.\",\n",
    "    \"This ruined what should have been a good day.\",\n",
    "    \"The wait was unbearable and unnecessary.\",\n",
    "    \"I found the whole thing incredibly tedious.\",\n",
    "    \"They clearly don't care about their customers.\",\n",
    "    \"I'm upset that this turned out so badly.\",\n",
    "    \"Everything that could go wrong did.\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset: {len(positive_sentences)} positive, {len(negative_sentences)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running layer comparison... (this will take a few minutes)\n",
      "\n",
      "Testing layer 0...\n",
      "Testing layer 7...\n",
      "Testing layer 14...\n",
      "Testing layer 21...\n",
      "Testing layer 27...\n",
      "\n",
      "============================================================\n",
      "RESULTS: Sentiment Detection by Layer (Qwen2.5-7B)\n",
      "============================================================\n",
      " layer  train_acc  test_acc      gap\n",
      "     0        1.0  0.714286 0.285714\n",
      "     7        1.0  1.000000 0.000000\n",
      "    14        1.0  1.000000 0.000000\n",
      "    21        1.0  1.000000 0.000000\n",
      "    27        1.0  1.000000 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Compare probes across layers\n",
    "# Qwen has 28 layers, so we test: early (0, 7), middle (14), late (21, 27)\n",
    "\n",
    "layers_to_test = [0, 7, 14, 21, 27]\n",
    "\n",
    "print(\"Running layer comparison... (this will take a few minutes)\\n\")\n",
    "results = extractor.compare_layers(\n",
    "    positive_sentences, \n",
    "    negative_sentences, \n",
    "    layers_to_test,\n",
    "    position='last'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS: Sentiment Detection by Layer (Qwen2.5-7B)\")\n",
    "print(\"=\" * 60)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best performing layer: 7 with 100.00% test accuracy\n"
     ]
    }
   ],
   "source": [
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by layer\n",
    "ax1.plot(results['layer'], results['train_acc'], marker='o', label='Train', linewidth=2)\n",
    "ax1.plot(results['layer'], results['test_acc'], marker='s', label='Test', linewidth=2)\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Sentiment Detection Accuracy by Layer (Qwen2.5-7B)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0.4, 1.05])\n",
    "\n",
    "# Plot 2: Train/test gap\n",
    "ax2.bar(results['layer'], results['gap'], color='coral', alpha=0.7)\n",
    "ax2.axhline(y=0.15, color='red', linestyle='--', label='15% threshold')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Train - Test Accuracy', fontsize=12)\n",
    "ax2.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('qwen_sentiment_probe_layer_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "best_layer = results.loc[results['test_acc'].idxmax(), 'layer']\n",
    "best_acc = results.loc[results['test_acc'].idxmax(), 'test_acc']\n",
    "print(f\"\\nBest performing layer: {best_layer} with {best_acc:.2%} test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different distribution: 10 positive, 10 negative\n"
     ]
    }
   ],
   "source": [
    "# Generalization test with different distribution\n",
    "different_positive = [\n",
    "    \"The mentorship programme completely changed my career trajectory.\",\n",
    "    \"I've never tasted homemade pasta this good before.\",\n",
    "    \"Seeing my daughter graduate was the proudest moment of my life.\",\n",
    "    \"This neighbourhood has such a wonderful sense of community.\",\n",
    "    \"The therapist really helped me work through my anxiety.\",\n",
    "    \"I'm blown away by how talented this band is.\",\n",
    "    \"The renovation turned out even better than we imagined.\",\n",
    "    \"Volunteering there has been incredibly rewarding.\",\n",
    "    \"I finally finished the marathon and it felt amazing.\",\n",
    "    \"The customer support team solved everything in one call.\",\n",
    "]\n",
    "\n",
    "different_negative = [\n",
    "    \"The dentist appointment was as painful as I feared.\",\n",
    "    \"I'm gutted that the concert got cancelled last minute.\",\n",
    "    \"This laptop overheats constantly and crashes without warning.\",\n",
    "    \"The landlord refuses to fix anything in this flat.\",\n",
    "    \"I felt completely blindsided by their decision.\",\n",
    "    \"The commute is draining the life out of me.\",\n",
    "    \"I'm heartbroken that the relationship ended this way.\",\n",
    "    \"The sequel completely failed to capture the original's magic.\",\n",
    "    \"I've been struggling to sleep properly for weeks.\",\n",
    "    \"The interview went terribly and I know I won't get the job.\",\n",
    "]\n",
    "\n",
    "print(f\"Different distribution: {len(different_positive)} positive, {len(different_negative)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best layer: 7\n",
      "\n",
      "=== Generalization Test (Qwen2.5-7B) ===\n",
      "Training accuracy: 100.00%\n",
      "Different distribution accuracy: 100.00%\n",
      "Accuracy change: 0.00%\n",
      "\n",
      "✓ Probe generalizes well! Setup verified.\n"
     ]
    }
   ],
   "source": [
    "# Train probe on original, test on different distribution\n",
    "# Use the best layer from previous analysis\n",
    "\n",
    "best_layer_idx = int(results.loc[results['test_acc'].idxmax(), 'layer'])\n",
    "print(f\"Using best layer: {best_layer_idx}\")\n",
    "\n",
    "# Get training data activations\n",
    "X_pos_train = extractor.get_batch_activations(positive_sentences, best_layer_idx)\n",
    "X_neg_train = extractor.get_batch_activations(negative_sentences, best_layer_idx)\n",
    "X_train = np.vstack([X_pos_train, X_neg_train])\n",
    "y_train = np.array([1] * len(positive_sentences) + [0] * len(negative_sentences))\n",
    "\n",
    "# Train probe\n",
    "probe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "probe.fit(X_train, y_train)\n",
    "train_acc = probe.score(X_train, y_train)\n",
    "\n",
    "# Get different distribution activations\n",
    "X_pos_diff = extractor.get_batch_activations(different_positive, best_layer_idx)\n",
    "X_neg_diff = extractor.get_batch_activations(different_negative, best_layer_idx)\n",
    "X_diff = np.vstack([X_pos_diff, X_neg_diff])\n",
    "y_diff = np.array([1] * len(different_positive) + [0] * len(different_negative))\n",
    "\n",
    "# Test on different distribution\n",
    "diff_acc = probe.score(X_diff, y_diff)\n",
    "\n",
    "print(f\"\\n=== Generalization Test (Qwen2.5-7B) ===\")\n",
    "print(f\"Training accuracy: {train_acc:.2%}\")\n",
    "print(f\"Different distribution accuracy: {diff_acc:.2%}\")\n",
    "print(f\"Accuracy change: {(diff_acc - train_acc):.2%}\")\n",
    "\n",
    "if diff_acc > 0.7:\n",
    "    print(\"\\n✓ Probe generalizes well! Setup verified.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Probe generalization is weak - may need investigation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: GPT-2 (TransformerLens) vs Qwen (nnsight)\n",
    "\n",
    "Record your results here:\n",
    "\n",
    "| Metric | GPT-2 (Day 3-4) | Qwen2.5-7B (Today) |\n",
    "|--------|-----------------|--------------------|\n",
    "| Best layer | ___ | ___ |\n",
    "| Best test accuracy | ___% | ___% |\n",
    "| Generalization accuracy | ___% | ___% |\n",
    "\n",
    "**Observations:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Troubleshooting & Success Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues\n",
    "\n",
    "**1. Out of Memory (OOM) Errors**\n",
    "- Reduce batch size (process fewer sentences at once)\n",
    "- Ensure using `torch_dtype=torch.float16`\n",
    "- Clear cache between operations: `torch.cuda.empty_cache()`\n",
    "\n",
    "**2. Slow Inference**\n",
    "- Check that model is on GPU: `next(model.parameters()).device`\n",
    "- Use `device_map=\"auto\"` for optimal placement\n",
    "- Consider using flash attention if available\n",
    "\n",
    "**3. Shape Mismatches**\n",
    "- Qwen has 28 layers (vs GPT-2's 12)\n",
    "- Qwen has 3584 hidden size (vs GPT-2's 768)\n",
    "- Always check `model.config` for correct dimensions\n",
    "\n",
    "**4. Vast.ai Specific**\n",
    "- Make sure SSH is configured correctly\n",
    "- Use persistent storage for model weights to avoid re-downloading\n",
    "- Check that you have enough disk space (~15GB for Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUCCESS CRITERIA CHECKLIST\n",
      "============================================================\n",
      "  [✓] Model loads without OOM\n",
      "  [✓] Can extract activations at any layer\n",
      "  [✓] Activations have correct shape\n",
      "  [✓] Probe achieves >70% test accuracy\n",
      "  [✓] Can generate text with activation access\n",
      "\n",
      "============================================================\n",
      "ALL CHECKS PASSED! Setup is verified.\n",
      "You're ready for the CoT understanding notebook.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Success criteria verification\n",
    "print(\"=\" * 60)\n",
    "print(\"SUCCESS CRITERIA CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = {\n",
    "    \"Model loads without OOM\": True,  # If you got here, it loaded\n",
    "    \"Can extract activations at any layer\": test_act.shape == (config.hidden_size,),\n",
    "    \"Activations have correct shape\": test_batch.shape == (3, config.hidden_size),\n",
    "    \"Probe achieves >70% test accuracy\": results['test_acc'].max() > 0.7,\n",
    "    \"Can generate text with activation access\": hidden_during_gen is not None,\n",
    "}\n",
    "\n",
    "for check, passed in checks.items():\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  [{status}] {check}\")\n",
    "\n",
    "if all(checks.values()):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ALL CHECKS PASSED! Setup is verified.\")\n",
    "    print(\"You're ready for the CoT understanding notebook.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n⚠ Some checks failed. Review the issues above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that nnsight setup is verified, you're ready for:\n",
    "\n",
    "1. **Understanding CoT Structure** (Day 8-9 continued)\n",
    "   - Analyze how Qwen generates reasoning\n",
    "   - Identify reasoning markers and structure\n",
    "\n",
    "2. **Building CoT Dataset** (Day 8-9 continued)\n",
    "   - Generate faithful/unfaithful CoT examples\n",
    "   - Create evaluation framework\n",
    "\n",
    "**Key concepts you now understand:**\n",
    "- Why nnsight (not TransformerLens) for reasoning models\n",
    "- How to extract activations with `.trace()` and `.save()`\n",
    "- How to intervene on activations (patching)\n",
    "- How to access activations during generation\n",
    "- Shape differences between different module outputs\n",
    "\n",
    "---\n",
    "\n",
    "**Save your work and record your results!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
