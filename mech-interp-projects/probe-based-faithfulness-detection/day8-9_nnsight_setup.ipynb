{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8-9: nnsight Setup with Qwen\n",
    "\n",
    "**Goal:** Learn nnsight fundamentals and verify setup with a probe verification exercise\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand why we're transitioning from TransformerLens to nnsight\n",
    "2. Load and interact with Qwen2.5-7B-Instruct using nnsight\n",
    "3. Extract activations from any layer\n",
    "4. Perform basic interventions (activation patching)\n",
    "5. Verify setup works by training a sentiment probe\n",
    "\n",
    "**Timeline:** 6-7 hours\n",
    "\n",
    "**Environment:** Vast.ai with GPU (16GB+ VRAM recommended)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why nnsight Instead of TransformerLens?\n",
    "\n",
    "In Days 3-6, you used **TransformerLens** with GPT-2 to learn probing fundamentals. Now we're switching to **nnsight** for your actual CoT faithfulness research.\n",
    "\n",
    "**The key difference:**\n",
    "\n",
    "| Aspect | TransformerLens | nnsight |\n",
    "|--------|-----------------|--------|\n",
    "| **Model support** | GPT-2, GPT-Neo, limited models | ANY HuggingFace model |\n",
    "| **Qwen/Llama/DeepSeek** | NOT SUPPORTED | Fully supported |\n",
    "| **CoT reasoning models** | Cannot use | Required for this |\n",
    "| **Best use case** | Learning mech interp basics | Production research on modern models |\n",
    "\n",
    "**Bottom line:** TransformerLens doesn't support Qwen, Llama, or other modern reasoning models. For CoT faithfulness research, you MUST use nnsight (or similar tools like nnsight).\n",
    "\n",
    "GPT-2 doesn't do meaningful chain-of-thought reasoning - it's too small. You need a model like Qwen2.5-7B-Instruct that actually reasons through problems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this first on a fresh Vast.ai instance)\n",
    "# Uncomment and run if packages aren't installed\n",
    "\n",
    "# !pip install nnsight>=0.3.0\n",
    "# !pip install torch>=2.0.0\n",
    "# !pip install transformers>=4.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install numpy scikit-learn matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    print(\"If on Vast.ai, make sure you selected a GPU instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# nnsight - the key library for this notebook\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading Qwen with nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen2.5-7B-Instruct with nnsight\n",
    "# This will download the model on first run (~14GB)\n",
    "\n",
    "print(\"Loading Qwen2.5-7B-Instruct... (this may take a few minutes on first run)\")\n",
    "\n",
    "model = LanguageModel(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    device_map=\"auto\",  # Automatically place on GPU\n",
    "    torch_dtype=torch.float16  # Use half precision to save memory\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model architecture\n",
    "# This is DIFFERENT from TransformerLens - we're working with HuggingFace structure\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key model properties for Qwen2.5-7B\n",
    "# Access the underlying HuggingFace model config\n",
    "\n",
    "config = model.config\n",
    "print(f\"Model: {config.model_type}\")\n",
    "print(f\"Hidden size (d_model): {config.hidden_size}\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Structural Differences from TransformerLens\n",
    "\n",
    "| TransformerLens (GPT-2) | nnsight (Qwen) |\n",
    "|------------------------|----------------|\n",
    "| `model.blocks[i]` | `model.model.layers[i]` |\n",
    "| `cache[\"resid_post\", layer]` | `model.model.layers[layer].output[0]` |\n",
    "| 12 layers, 768 hidden | 28 layers, 3584 hidden |\n",
    "| Automatic caching | Explicit `.save()` required |\n",
    "\n",
    "The nnsight library wraps the HuggingFace model, so we access layers through `model.model.layers[i]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Basic Tracing - Saving Activations\n",
    "\n",
    "The core concept in nnsight is the **trace context**. Inside a `with model.trace()` block:\n",
    "- Operations create \"proxy\" objects\n",
    "- Nothing executes until the context exits\n",
    "- Use `.save()` to preserve values you want to access later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest example: save hidden states at one layer\n",
    "\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    # Access layer 14 (middle of the 28 layers)\n",
    "    # .output[0] gets the hidden states (first element of output tuple)\n",
    "    hidden_states = model.model.layers[14].output[0].save()\n",
    "\n",
    "# After the context exits, hidden_states.value contains the actual tensor\n",
    "print(f\"Hidden states shape: {hidden_states.value.shape}\")\n",
    "print(f\"Expected: [batch=1, seq_len, hidden_size={config.hidden_size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the shape:\n",
    "# - Batch dimension (1 because we passed a single string)\n",
    "# - Sequence length (number of tokens in \"Hello, world!\")\n",
    "# - Hidden dimension (3584 for Qwen2.5-7B)\n",
    "\n",
    "# Let's check the tokenization\n",
    "tokens = model.tokenizer.encode(\"Hello, world!\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token strings: {[model.tokenizer.decode([t]) for t in tokens]}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save activations at MULTIPLE layers\n",
    "\n",
    "layers_to_extract = [0, 7, 14, 21, 27]  # Early, mid, late layers\n",
    "saved_activations = {}\n",
    "\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "    for layer in layers_to_extract:\n",
    "        saved_activations[layer] = model.model.layers[layer].output[0].save()\n",
    "\n",
    "# Check shapes\n",
    "print(\"Activations at each layer:\")\n",
    "for layer, acts in saved_activations.items():\n",
    "    print(f\"  Layer {layer:2d}: shape = {acts.value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Extract final token activation at specified layer\n",
    "# This mirrors what we did in TransformerLens\n",
    "\n",
    "def get_final_token_activation_nnsight(model, text, layer):\n",
    "    \"\"\"\n",
    "    Extract the activation of the final token at a specified layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string\n",
    "        layer: Layer number (0 to num_layers-1)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    with model.trace(text):\n",
    "        hidden = model.model.layers[layer].output[0].save()\n",
    "    \n",
    "    # Get final token's activation: [batch=0, position=-1, :]\n",
    "    final_act = hidden.value[0, -1, :].cpu().numpy()\n",
    "    return final_act\n",
    "\n",
    "# Test it\n",
    "test_act = get_final_token_activation_nnsight(model, \"Test sentence.\", layer=14)\n",
    "print(f\"Activation shape: {test_act.shape}\")\n",
    "print(f\"Expected: ({config.hidden_size},)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: TransformerLens vs nnsight Activation Extraction\n",
    "\n",
    "**TransformerLens:**\n",
    "```python\n",
    "_, cache = model.run_with_cache(text)\n",
    "activation = cache[\"resid_post\", layer][0, -1, :].cpu().numpy()\n",
    "```\n",
    "\n",
    "**nnsight:**\n",
    "```python\n",
    "with model.trace(text):\n",
    "    hidden = model.model.layers[layer].output[0].save()\n",
    "activation = hidden.value[0, -1, :].cpu().numpy()\n",
    "```\n",
    "\n",
    "Key differences:\n",
    "1. nnsight requires explicit `.save()` - values aren't automatically cached\n",
    "2. nnsight uses deferred execution - operations run when context exits\n",
    "3. nnsight uses HuggingFace model structure (`model.model.layers`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3.5: Accessing Attention and MLP Components\n",
    "\n",
    "In Day 5-6, you learned that different components (attention vs MLP) serve different computational roles:\n",
    "- **Attention:** Routes information between positions (\"where should I look?\")\n",
    "- **MLP:** Transforms information within position (\"what should I compute?\")\n",
    "\n",
    "Here's how to access these components in nnsight (compared to TransformerLens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's explore the structure of a single layer in Qwen\n",
    "# This shows us what submodules we can access\n",
    "\n",
    "print(\"Structure of layer 0:\")\n",
    "print(\"=\" * 50)\n",
    "for name, module in model.model.layers[0].named_children():\n",
    "    print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access MLP output at a specific layer\n",
    "# The MLP in Qwen is called \"mlp\" (in GPT-2/TransformerLens it was also \"mlp\")\n",
    "\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "    # Get the MLP output (after the MLP computation, before adding to residual)\n",
    "    mlp_output = model.model.layers[14].mlp.output.save()\n",
    "    \n",
    "    # For comparison, also get the full layer output (residual stream)\n",
    "    layer_output = model.model.layers[14].output[0].save()\n",
    "\n",
    "print(f\"MLP output shape: {mlp_output.value.shape}\")\n",
    "print(f\"Layer output shape: {layer_output.value.shape}\")\n",
    "print(f\"\\nBoth should have hidden_size={config.hidden_size} as last dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Attention output at a specific layer\n",
    "# In Qwen, the self-attention module is called \"self_attn\"\n",
    "\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "    # Get the attention output (after attention computation, before adding to residual)\n",
    "    # Note: attention output is typically a tuple, we want the first element\n",
    "    attn_output = model.model.layers[14].self_attn.output[0].save()\n",
    "\n",
    "print(f\"Attention output shape: {attn_output.value.shape}\")\n",
    "print(f\"Expected: [batch, seq_len, hidden_size={config.hidden_size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Attention PATTERNS (the attention weights after softmax)\n",
    "# This requires accessing internal attention computation\n",
    "# We need to enable output_attentions in the model config\n",
    "\n",
    "# Method 1: Access attention weights by modifying the forward call\n",
    "# This is more involved - we need to look at what the attention module outputs\n",
    "\n",
    "# Let's check what the self_attn module returns\n",
    "with model.trace(\"Hello world\"):\n",
    "    # The full output of self_attn - let's see what's there\n",
    "    full_attn_output = model.model.layers[14].self_attn.output.save()\n",
    "\n",
    "print(f\"Attention module output type: {type(full_attn_output.value)}\")\n",
    "if isinstance(full_attn_output.value, tuple):\n",
    "    print(f\"Number of elements in tuple: {len(full_attn_output.value)}\")\n",
    "    for i, elem in enumerate(full_attn_output.value):\n",
    "        if elem is not None:\n",
    "            print(f\"  Element {i}: shape = {elem.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get attention patterns (weights), we need to enable output_attentions\n",
    "# This makes the model return attention weights as part of its output\n",
    "\n",
    "# Temporarily modify config to output attentions\n",
    "original_output_attentions = model.config.output_attentions\n",
    "model.config.output_attentions = True\n",
    "\n",
    "with model.trace(\"The capital of France is Paris.\"):\n",
    "    # Now attention weights should be in the output\n",
    "    attn_with_weights = model.model.layers[14].self_attn.output.save()\n",
    "\n",
    "# Restore original setting\n",
    "model.config.output_attentions = original_output_attentions\n",
    "\n",
    "# Check what we got\n",
    "print(f\"Output type: {type(attn_with_weights.value)}\")\n",
    "if isinstance(attn_with_weights.value, tuple):\n",
    "    print(f\"Number of elements: {len(attn_with_weights.value)}\")\n",
    "    for i, elem in enumerate(attn_with_weights.value):\n",
    "        if elem is not None:\n",
    "            print(f\"  Element {i}: shape = {elem.shape}\")\n",
    "            if len(elem.shape) == 4:  # [batch, num_heads, seq, seq]\n",
    "                print(f\"    -> This is likely the attention pattern!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for extracting different components\n",
    "\n",
    "def get_mlp_output(model, text, layer, position='last'):\n",
    "    \"\"\"\n",
    "    Extract MLP output at a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string\n",
    "        layer: Layer number\n",
    "        position: 'last', 'first', 'mean', or int\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    with model.trace(text):\n",
    "        mlp_out = model.model.layers[layer].mlp.output.save()\n",
    "    \n",
    "    acts = mlp_out.value[0]  # Remove batch dim\n",
    "    \n",
    "    if position == 'last':\n",
    "        return acts[-1, :].cpu().numpy()\n",
    "    elif position == 'first':\n",
    "        return acts[0, :].cpu().numpy()\n",
    "    elif position == 'mean':\n",
    "        return acts.mean(dim=0).cpu().numpy()\n",
    "    elif isinstance(position, int):\n",
    "        return acts[position, :].cpu().numpy()\n",
    "\n",
    "\n",
    "def get_attention_output(model, text, layer, position='last'):\n",
    "    \"\"\"\n",
    "    Extract attention output at a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string  \n",
    "        layer: Layer number\n",
    "        position: 'last', 'first', 'mean', or int\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    with model.trace(text):\n",
    "        attn_out = model.model.layers[layer].self_attn.output[0].save()\n",
    "    \n",
    "    acts = attn_out.value[0]  # Remove batch dim\n",
    "    \n",
    "    if position == 'last':\n",
    "        return acts[-1, :].cpu().numpy()\n",
    "    elif position == 'first':\n",
    "        return acts[0, :].cpu().numpy()\n",
    "    elif position == 'mean':\n",
    "        return acts.mean(dim=0).cpu().numpy()\n",
    "    elif isinstance(position, int):\n",
    "        return acts[position, :].cpu().numpy()\n",
    "\n",
    "\n",
    "def get_attention_patterns(model, text, layer):\n",
    "    \"\"\"\n",
    "    Extract attention patterns (weights after softmax) at a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: nnsight LanguageModel\n",
    "        text: Input string\n",
    "        layer: Layer number\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Enable attention output temporarily\n",
    "    original = model.config.output_attentions\n",
    "    model.config.output_attentions = True\n",
    "    \n",
    "    with model.trace(text):\n",
    "        attn_out = model.model.layers[layer].self_attn.output.save()\n",
    "    \n",
    "    model.config.output_attentions = original\n",
    "    \n",
    "    # Attention weights are typically the second element of the tuple\n",
    "    # Shape: [batch, num_heads, seq_len, seq_len]\n",
    "    if isinstance(attn_out.value, tuple) and len(attn_out.value) > 1:\n",
    "        patterns = attn_out.value[1]\n",
    "        if patterns is not None:\n",
    "            return patterns[0].cpu().numpy()  # Remove batch dim\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Test the helper functions\n",
    "print(\"Testing component extraction functions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_mlp = get_mlp_output(model, \"Test sentence.\", layer=14, position='last')\n",
    "print(f\"MLP output shape: {test_mlp.shape}\")\n",
    "\n",
    "test_attn = get_attention_output(model, \"Test sentence.\", layer=14, position='last')\n",
    "print(f\"Attention output shape: {test_attn.shape}\")\n",
    "\n",
    "test_patterns = get_attention_patterns(model, \"Test sentence.\", layer=14)\n",
    "if test_patterns is not None:\n",
    "    print(f\"Attention patterns shape: {test_patterns.shape}\")\n",
    "    print(f\"  -> [num_heads={test_patterns.shape[0]}, seq_len={test_patterns.shape[1]}, seq_len={test_patterns.shape[2]}]\")\n",
    "else:\n",
    "    print(\"Attention patterns: Not available (model may not support output_attentions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Access Comparison: TransformerLens vs nnsight\n",
    "\n",
    "| Component | TransformerLens | nnsight (Qwen) |\n",
    "|-----------|-----------------|----------------|\n",
    "| **Residual stream (layer output)** | `cache[\"resid_post\", layer]` | `model.model.layers[layer].output[0]` |\n",
    "| **MLP output** | `cache[\"blocks.{layer}.hook_mlp_out\"]` | `model.model.layers[layer].mlp.output` |\n",
    "| **Attention output** | `cache[\"attn_out\", layer]` | `model.model.layers[layer].self_attn.output[0]` |\n",
    "| **Attention patterns** | `cache[\"blocks.{layer}.attn.hook_pattern\"]` | Enable `output_attentions=True`, then `self_attn.output[1]` |\n",
    "| **Individual head outputs** | `cache[\"blocks.{layer}.attn.hook_z\"]` | Requires accessing internal attention computation |\n",
    "\n",
    "**Key insight from Day 5-6:** You found that middle-layer attention heads outperformed residual stream probes for sentiment. With these extraction functions, you can test if the same pattern holds for Qwen and for faithfulness detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison: Residual vs MLP vs Attention at the same layer\n",
    "# This mirrors the analysis you did in Day 5-6\n",
    "\n",
    "test_text = \"I love this movie!\"\n",
    "layer = 14\n",
    "\n",
    "# Extract all three components\n",
    "with model.trace(test_text):\n",
    "    residual = model.model.layers[layer].output[0].save()\n",
    "    mlp = model.model.layers[layer].mlp.output.save()\n",
    "    attn = model.model.layers[layer].self_attn.output[0].save()\n",
    "\n",
    "# Get final token activations\n",
    "residual_act = residual.value[0, -1, :].cpu().numpy()\n",
    "mlp_act = mlp.value[0, -1, :].cpu().numpy()\n",
    "attn_act = attn.value[0, -1, :].cpu().numpy()\n",
    "\n",
    "print(f\"Component activations at layer {layer}, final token:\")\n",
    "print(f\"  Residual stream: norm = {np.linalg.norm(residual_act):.2f}\")\n",
    "print(f\"  MLP output:      norm = {np.linalg.norm(mlp_act):.2f}\")\n",
    "print(f\"  Attention output: norm = {np.linalg.norm(attn_act):.2f}\")\n",
    "\n",
    "# Note: residual ≈ previous_residual + attention + mlp\n",
    "# So residual has accumulated information from both components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Generation with Activation Access\n",
    "\n",
    "For CoT faithfulness research, you need to:\n",
    "1. Generate reasoning (multi-token output)\n",
    "2. Access activations during that generation\n",
    "\n",
    "nnsight uses `.generate()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with activation access\n",
    "\n",
    "prompt = \"What is 17 * 23? Let me think step by step.\"\n",
    "\n",
    "with model.generate(max_new_tokens=150) as generator:\n",
    "    with generator.invoke(prompt):\n",
    "        # Save activations at layer 14 during generation\n",
    "        hidden_during_gen = model.model.layers[14].output[0].save()\n",
    "\n",
    "# Decode the generated output\n",
    "output_tokens = generator.output[0]\n",
    "output_text = model.tokenizer.decode(output_tokens)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(\"=\" * 50)\n",
    "print(output_text)\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nActivations shape: {hidden_during_gen.value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the activation shape during generation:\n",
    "# The shape is [batch, total_sequence_length, hidden_size]\n",
    "# total_sequence_length = prompt_tokens + generated_tokens\n",
    "\n",
    "prompt_tokens = model.tokenizer.encode(prompt)\n",
    "print(f\"Prompt tokens: {len(prompt_tokens)}\")\n",
    "print(f\"Total sequence length in activations: {hidden_during_gen.value.shape[1]}\")\n",
    "print(f\"Generated tokens: {hidden_during_gen.value.shape[1] - len(prompt_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tokens to their activations\n",
    "# This is important for position analysis in faithfulness research\n",
    "\n",
    "all_token_ids = output_tokens.tolist()\n",
    "all_tokens = [model.tokenizer.decode([t]) for t in all_token_ids]\n",
    "\n",
    "print(\"Token-Activation mapping (first 20 tokens):\")\n",
    "print(\"-\" * 60)\n",
    "for i, (tok_id, tok_str) in enumerate(zip(all_token_ids[:20], all_tokens[:20])):\n",
    "    act_norm = hidden_during_gen.value[0, i, :].norm().item()\n",
    "    print(f\"Pos {i:3d}: '{tok_str:15s}' | Activation norm: {act_norm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Intervention Basics\n",
    "\n",
    "Beyond reading activations, nnsight lets you **modify** them during forward pass. This is activation patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple intervention: zero out the last token's activation at layer 10\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# First, get baseline output\n",
    "with model.generate(max_new_tokens=5) as generator:\n",
    "    with generator.invoke(prompt):\n",
    "        pass  # No intervention\n",
    "\n",
    "baseline_output = model.tokenizer.decode(generator.output[0])\n",
    "print(f\"Baseline: {baseline_output}\")\n",
    "\n",
    "# Now with intervention\n",
    "with model.generate(max_new_tokens=5) as generator:\n",
    "    with generator.invoke(prompt):\n",
    "        # Zero out the last position at layer 10\n",
    "        model.model.layers[10].output[0][:, -1, :] = 0\n",
    "\n",
    "intervened_output = model.tokenizer.decode(generator.output[0])\n",
    "print(f\"Intervened (zero layer 10): {intervened_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-prompt intervention (activation patching)\n",
    "# Extract activation from one prompt, inject into another\n",
    "\n",
    "source_prompt = \"The capital of Germany is\"\n",
    "target_prompt = \"The capital of France is\"\n",
    "\n",
    "# Step 1: Extract activation from source\n",
    "with model.trace(source_prompt):\n",
    "    source_activation = model.model.layers[14].output[0].save()\n",
    "\n",
    "print(f\"Source activation shape: {source_activation.value.shape}\")\n",
    "\n",
    "# Step 2: Inject into target and generate\n",
    "with model.generate(max_new_tokens=5) as generator:\n",
    "    with generator.invoke(target_prompt):\n",
    "        # Replace layer 14 activation with source\n",
    "        model.model.layers[14].output[0][:, :, :] = source_activation.value\n",
    "\n",
    "patched_output = model.tokenizer.decode(generator.output[0])\n",
    "print(f\"\\nTarget prompt: '{target_prompt}'\")\n",
    "print(f\"Patched with source: '{source_prompt}'\")\n",
    "print(f\"Output: {patched_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: TransformerLens vs nnsight Interventions\n",
    "\n",
    "**TransformerLens (hook-based):**\n",
    "```python\n",
    "def hook_fn(activation, hook):\n",
    "    activation[:, -1, :] = 0\n",
    "    return activation\n",
    "\n",
    "model.run_with_hooks(prompt, fwd_hooks=[(\"blocks.10.hook_resid_post\", hook_fn)])\n",
    "```\n",
    "\n",
    "**nnsight (assignment-based):**\n",
    "```python\n",
    "with model.trace(prompt):\n",
    "    model.model.layers[10].output[0][:, -1, :] = 0\n",
    "```\n",
    "\n",
    "nnsight's approach is more intuitive - you just assign to the values you want to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Building Probe-Compatible Extraction Class\n",
    "\n",
    "Let's build a reusable class that mirrors the ProbeToolkit from TransformerLens days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNsightActivationExtractor:\n",
    "    \"\"\"\n",
    "    Reusable toolkit for extracting activations from nnsight models.\n",
    "    Mirrors the ProbeToolkit from TransformerLens exercises.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.config = model.config\n",
    "        self.num_layers = model.config.num_hidden_layers\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "    \n",
    "    def get_layer_activation(self, text, layer, position='last'):\n",
    "        \"\"\"\n",
    "        Extract activation at a specific layer and position.\n",
    "        \n",
    "        Args:\n",
    "            text: Input string\n",
    "            layer: Layer number (0 to num_layers-1)\n",
    "            position: 'last', 'first', 'mean', or int for specific position\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (hidden_size,)\n",
    "        \"\"\"\n",
    "        with self.model.trace(text):\n",
    "            hidden = self.model.model.layers[layer].output[0].save()\n",
    "        \n",
    "        acts = hidden.value[0]  # Remove batch dimension\n",
    "        \n",
    "        if position == 'last':\n",
    "            return acts[-1, :].cpu().numpy()\n",
    "        elif position == 'first':\n",
    "            return acts[0, :].cpu().numpy()\n",
    "        elif position == 'mean':\n",
    "            return acts.mean(dim=0).cpu().numpy()\n",
    "        elif isinstance(position, int):\n",
    "            return acts[position, :].cpu().numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown position: {position}\")\n",
    "    \n",
    "    def get_batch_activations(self, texts, layer, position='last'):\n",
    "        \"\"\"\n",
    "        Extract activations for multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input strings\n",
    "            layer: Layer number\n",
    "            position: Position strategy\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (n_texts, hidden_size)\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        for text in texts:\n",
    "            act = self.get_layer_activation(text, layer, position)\n",
    "            activations.append(act)\n",
    "        return np.array(activations)\n",
    "    \n",
    "    def compare_layers(self, texts_pos, texts_neg, layers, position='last'):\n",
    "        \"\"\"\n",
    "        Compare probe performance across multiple layers.\n",
    "        \n",
    "        Args:\n",
    "            texts_pos: List of positive class texts\n",
    "            texts_neg: List of negative class texts\n",
    "            layers: List of layer numbers to test\n",
    "            position: Position strategy\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with layer, train_acc, test_acc\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            print(f\"Testing layer {layer}...\")\n",
    "            \n",
    "            # Extract activations\n",
    "            X_pos = self.get_batch_activations(texts_pos, layer, position)\n",
    "            X_neg = self.get_batch_activations(texts_neg, layer, position)\n",
    "            \n",
    "            # Combine into dataset\n",
    "            X = np.vstack([X_pos, X_neg])\n",
    "            y = np.array([1] * len(texts_pos) + [0] * len(texts_neg))\n",
    "            \n",
    "            # Train/test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.3, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Train probe\n",
    "            probe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            probe.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = probe.score(X_train, y_train)\n",
    "            test_acc = probe.score(X_test, y_test)\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'gap': train_acc - test_acc\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Create extractor instance\n",
    "extractor = NNsightActivationExtractor(model)\n",
    "print(f\"Extractor ready for model with {extractor.num_layers} layers, {extractor.hidden_size} hidden dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extractor\n",
    "test_act = extractor.get_layer_activation(\"Test sentence.\", layer=14, position='last')\n",
    "print(f\"Single activation shape: {test_act.shape}\")\n",
    "\n",
    "test_batch = extractor.get_batch_activations(\n",
    "    [\"First test.\", \"Second test.\", \"Third test.\"],\n",
    "    layer=14,\n",
    "    position='last'\n",
    ")\n",
    "print(f\"Batch activation shape: {test_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Full Probe Verification\n",
    "\n",
    "Let's verify our setup by training the same sentiment probe from Day 3-4 on Qwen activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same sentiment dataset from Day 3-4\n",
    "positive_sentences = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is amazing and wonderful!\",\n",
    "    \"Great job, fantastic work!\",\n",
    "    \"I absolutely loved every minute of that film.\",\n",
    "    \"This is the best coffee I've ever tasted.\",\n",
    "    \"She was so kind and helpful throughout the process.\",\n",
    "    \"What a beautiful day to be outside.\",\n",
    "    \"I'm thrilled with how the project turned out.\",\n",
    "    \"The team did an outstanding job on this.\",\n",
    "    \"I can't wait to visit again next year.\",\n",
    "    \"This restaurant exceeded all my expectations.\",\n",
    "    \"He's such a talented and generous person.\",\n",
    "    \"I feel incredibly grateful for this opportunity.\",\n",
    "    \"The service here is always fantastic.\",\n",
    "    \"That was the most fun I've had in ages.\",\n",
    "    \"I'm so proud of what we accomplished together.\",\n",
    "    \"This book changed my perspective completely.\",\n",
    "    \"The sunset tonight was absolutely stunning.\",\n",
    "    \"I've never felt more welcomed anywhere.\",\n",
    "    \"Everything about this experience was delightful.\",\n",
    "    \"She gave the most inspiring speech I've ever heard.\",\n",
    "    \"I'm genuinely excited about what's next.\",\n",
    "    \"This made my whole week better.\",\n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"I hate this movie.\",\n",
    "    \"This is terrible and awful.\",\n",
    "    \"Poor job, disappointing work.\",\n",
    "    \"I was deeply disappointed by the outcome.\",\n",
    "    \"The food was cold and tasteless.\",\n",
    "    \"This has been the worst customer service experience.\",\n",
    "    \"I regret wasting my time on this.\",\n",
    "    \"The whole event was a complete disaster.\",\n",
    "    \"I'm frustrated with how poorly this was handled.\",\n",
    "    \"Nothing about this met my expectations.\",\n",
    "    \"The quality has really gone downhill.\",\n",
    "    \"I felt completely ignored the entire time.\",\n",
    "    \"This product broke after just one use.\",\n",
    "    \"What a miserable waste of money.\",\n",
    "    \"I've never been so let down by a company.\",\n",
    "    \"The atmosphere was unpleasant and unwelcoming.\",\n",
    "    \"I'm annoyed that nobody bothered to help.\",\n",
    "    \"This ruined what should have been a good day.\",\n",
    "    \"The wait was unbearable and unnecessary.\",\n",
    "    \"I found the whole thing incredibly tedious.\",\n",
    "    \"They clearly don't care about their customers.\",\n",
    "    \"I'm upset that this turned out so badly.\",\n",
    "    \"Everything that could go wrong did.\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset: {len(positive_sentences)} positive, {len(negative_sentences)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare probes across layers\n",
    "# Qwen has 28 layers, so we test: early (0, 7), middle (14), late (21, 27)\n",
    "\n",
    "layers_to_test = [0, 7, 14, 21, 27]\n",
    "\n",
    "print(\"Running layer comparison... (this will take a few minutes)\\n\")\n",
    "results = extractor.compare_layers(\n",
    "    positive_sentences, \n",
    "    negative_sentences, \n",
    "    layers_to_test,\n",
    "    position='last'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS: Sentiment Detection by Layer (Qwen2.5-7B)\")\n",
    "print(\"=\" * 60)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by layer\n",
    "ax1.plot(results['layer'], results['train_acc'], marker='o', label='Train', linewidth=2)\n",
    "ax1.plot(results['layer'], results['test_acc'], marker='s', label='Test', linewidth=2)\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Sentiment Detection Accuracy by Layer (Qwen2.5-7B)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0.4, 1.05])\n",
    "\n",
    "# Plot 2: Train/test gap\n",
    "ax2.bar(results['layer'], results['gap'], color='coral', alpha=0.7)\n",
    "ax2.axhline(y=0.15, color='red', linestyle='--', label='15% threshold')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Train - Test Accuracy', fontsize=12)\n",
    "ax2.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('qwen_sentiment_probe_layer_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "best_layer = results.loc[results['test_acc'].idxmax(), 'layer']\n",
    "best_acc = results.loc[results['test_acc'].idxmax(), 'test_acc']\n",
    "print(f\"\\nBest performing layer: {best_layer} with {best_acc:.2%} test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization test with different distribution\n",
    "different_positive = [\n",
    "    \"The mentorship programme completely changed my career trajectory.\",\n",
    "    \"I've never tasted homemade pasta this good before.\",\n",
    "    \"Seeing my daughter graduate was the proudest moment of my life.\",\n",
    "    \"This neighbourhood has such a wonderful sense of community.\",\n",
    "    \"The therapist really helped me work through my anxiety.\",\n",
    "    \"I'm blown away by how talented this band is.\",\n",
    "    \"The renovation turned out even better than we imagined.\",\n",
    "    \"Volunteering there has been incredibly rewarding.\",\n",
    "    \"I finally finished the marathon and it felt amazing.\",\n",
    "    \"The customer support team solved everything in one call.\",\n",
    "]\n",
    "\n",
    "different_negative = [\n",
    "    \"The dentist appointment was as painful as I feared.\",\n",
    "    \"I'm gutted that the concert got cancelled last minute.\",\n",
    "    \"This laptop overheats constantly and crashes without warning.\",\n",
    "    \"The landlord refuses to fix anything in this flat.\",\n",
    "    \"I felt completely blindsided by their decision.\",\n",
    "    \"The commute is draining the life out of me.\",\n",
    "    \"I'm heartbroken that the relationship ended this way.\",\n",
    "    \"The sequel completely failed to capture the original's magic.\",\n",
    "    \"I've been struggling to sleep properly for weeks.\",\n",
    "    \"The interview went terribly and I know I won't get the job.\",\n",
    "]\n",
    "\n",
    "print(f\"Different distribution: {len(different_positive)} positive, {len(different_negative)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probe on original, test on different distribution\n",
    "# Use the best layer from previous analysis\n",
    "\n",
    "best_layer_idx = int(results.loc[results['test_acc'].idxmax(), 'layer'])\n",
    "print(f\"Using best layer: {best_layer_idx}\")\n",
    "\n",
    "# Get training data activations\n",
    "X_pos_train = extractor.get_batch_activations(positive_sentences, best_layer_idx)\n",
    "X_neg_train = extractor.get_batch_activations(negative_sentences, best_layer_idx)\n",
    "X_train = np.vstack([X_pos_train, X_neg_train])\n",
    "y_train = np.array([1] * len(positive_sentences) + [0] * len(negative_sentences))\n",
    "\n",
    "# Train probe\n",
    "probe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "probe.fit(X_train, y_train)\n",
    "train_acc = probe.score(X_train, y_train)\n",
    "\n",
    "# Get different distribution activations\n",
    "X_pos_diff = extractor.get_batch_activations(different_positive, best_layer_idx)\n",
    "X_neg_diff = extractor.get_batch_activations(different_negative, best_layer_idx)\n",
    "X_diff = np.vstack([X_pos_diff, X_neg_diff])\n",
    "y_diff = np.array([1] * len(different_positive) + [0] * len(different_negative))\n",
    "\n",
    "# Test on different distribution\n",
    "diff_acc = probe.score(X_diff, y_diff)\n",
    "\n",
    "print(f\"\\n=== Generalization Test (Qwen2.5-7B) ===\")\n",
    "print(f\"Training accuracy: {train_acc:.2%}\")\n",
    "print(f\"Different distribution accuracy: {diff_acc:.2%}\")\n",
    "print(f\"Accuracy change: {(diff_acc - train_acc):.2%}\")\n",
    "\n",
    "if diff_acc > 0.7:\n",
    "    print(\"\\n✓ Probe generalizes well! Setup verified.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Probe generalization is weak - may need investigation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: GPT-2 (TransformerLens) vs Qwen (nnsight)\n",
    "\n",
    "Record your results here:\n",
    "\n",
    "| Metric | GPT-2 (Day 3-4) | Qwen2.5-7B (Today) |\n",
    "|--------|-----------------|--------------------|\n",
    "| Best layer | ___ | ___ |\n",
    "| Best test accuracy | ___% | ___% |\n",
    "| Generalization accuracy | ___% | ___% |\n",
    "\n",
    "**Observations:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Troubleshooting & Success Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues\n",
    "\n",
    "**1. Out of Memory (OOM) Errors**\n",
    "- Reduce batch size (process fewer sentences at once)\n",
    "- Ensure using `torch_dtype=torch.float16`\n",
    "- Clear cache between operations: `torch.cuda.empty_cache()`\n",
    "\n",
    "**2. Slow Inference**\n",
    "- Check that model is on GPU: `next(model.parameters()).device`\n",
    "- Use `device_map=\"auto\"` for optimal placement\n",
    "- Consider using flash attention if available\n",
    "\n",
    "**3. Shape Mismatches**\n",
    "- Qwen has 28 layers (vs GPT-2's 12)\n",
    "- Qwen has 3584 hidden size (vs GPT-2's 768)\n",
    "- Always check `model.config` for correct dimensions\n",
    "\n",
    "**4. Vast.ai Specific**\n",
    "- Make sure SSH is configured correctly\n",
    "- Use persistent storage for model weights to avoid re-downloading\n",
    "- Check that you have enough disk space (~15GB for Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success criteria verification\n",
    "print(\"=\" * 60)\n",
    "print(\"SUCCESS CRITERIA CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = {\n",
    "    \"Model loads without OOM\": True,  # If you got here, it loaded\n",
    "    \"Can extract activations at any layer\": test_act.shape == (config.hidden_size,),\n",
    "    \"Activations have correct shape\": test_batch.shape == (3, config.hidden_size),\n",
    "    \"Probe achieves >70% test accuracy\": results['test_acc'].max() > 0.7,\n",
    "    \"Can generate text with activation access\": hidden_during_gen.value is not None,\n",
    "}\n",
    "\n",
    "for check, passed in checks.items():\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  [{status}] {check}\")\n",
    "\n",
    "if all(checks.values()):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ALL CHECKS PASSED! Setup is verified.\")\n",
    "    print(\"You're ready for the CoT understanding notebook.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n⚠ Some checks failed. Review the issues above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that nnsight setup is verified, you're ready for:\n",
    "\n",
    "1. **Understanding CoT Structure** (Day 8-9 continued)\n",
    "   - Analyze how Qwen generates reasoning\n",
    "   - Identify reasoning markers and structure\n",
    "\n",
    "2. **Building CoT Dataset** (Day 8-9 continued)\n",
    "   - Generate faithful/unfaithful CoT examples\n",
    "   - Create evaluation framework\n",
    "\n",
    "**Key concepts you now understand:**\n",
    "- Why nnsight (not TransformerLens) for reasoning models\n",
    "- How to extract activations with `.trace()` and `.save()`\n",
    "- How to intervene on activations (patching)\n",
    "- How to access activations during generation\n",
    "\n",
    "---\n",
    "\n",
    "**Save your work and record your results!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
