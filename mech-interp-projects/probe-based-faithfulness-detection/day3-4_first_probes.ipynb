{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3-4: First Probes - Sentiment Detection\n",
    "\n",
    "**Goal:** Build hands-on probe expertise through sentiment classification\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Train your first probe on transformer activations\n",
    "2. Compare probe performance across layers\n",
    "3. Test generalization and identify failure modes\n",
    "4. Understand spurious correlations\n",
    "\n",
    "**Timeline:** 8-10 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import transformer_lens as tl\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"TransformerLens ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small\n",
    "model = tl.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "print(f\"Model loaded: {model.cfg.n_layers} layers, {model.cfg.d_model} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic Sentiment Probe (2 hours)\n",
    "\n",
    "**Concept:** Can we train a probe to detect sentiment from layer activations?\n",
    "\n",
    "**Hypothesis:** If sentiment information is linearly accessible at layer 6, a simple logistic regression should achieve >70% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Positive and negative sentences\n",
    "positive_sentences = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is amazing and wonderful!\",\n",
    "    \"Great job, fantastic work!\",\n",
    "    \"Absolutely brilliant performance!\",\n",
    "    \"I'm so happy with the results!\",\n",
    "    \"This exceeded all my expectations!\",\n",
    "    \"Wonderful experience, highly recommend!\",\n",
    "    \"Best decision I ever made!\",\n",
    "    \"This is perfect in every way!\",\n",
    "    \"I'm thrilled with how this turned out!\",\n",
    "    \"Outstanding quality and service!\",\n",
    "    \"This brings me so much joy!\",\n",
    "    \"Incredible work, truly impressive!\",\n",
    "    \"I'm delighted with this purchase!\",\n",
    "    \"This is exactly what I needed!\",\n",
    "    \"Five stars, absolutely love it!\",\n",
    "    \"This made my day so much better!\",\n",
    "    \"I'm grateful for this opportunity!\",\n",
    "    \"This is surprisingly excellent!\",\n",
    "    \"I'm really pleased with the outcome!\",\n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"I hate this movie.\",\n",
    "    \"This is terrible and awful.\",\n",
    "    \"Poor job, disappointing work.\",\n",
    "    \"Absolutely dreadful performance.\",\n",
    "    \"I'm so unhappy with the results.\",\n",
    "    \"This failed all my expectations.\",\n",
    "    \"Terrible experience, avoid this.\",\n",
    "    \"Worst decision I ever made.\",\n",
    "    \"This is flawed in every way.\",\n",
    "    \"I'm upset with how this turned out.\",\n",
    "    \"Poor quality and bad service.\",\n",
    "    \"This brings me so much frustration.\",\n",
    "    \"Mediocre work, not impressive.\",\n",
    "    \"I'm disappointed with this purchase.\",\n",
    "    \"This is not what I needed.\",\n",
    "    \"One star, completely hate it.\",\n",
    "    \"This ruined my day completely.\",\n",
    "    \"I regret this opportunity.\",\n",
    "    \"This is surprisingly horrible.\",\n",
    "    \"I'm really displeased with the outcome.\",\n",
    "]\n",
    "\n",
    "print(f\"Dataset: {len(positive_sentences)} positive, {len(negative_sentences)} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Activations\n",
    "\n",
    "**Key Decision:** Which token's activation should we use?\n",
    "- **Last token:** Usually contains most information (what we'll start with)\n",
    "- First token: Less information about sentence content\n",
    "- Mean pooling: Average across all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_token_activation(model, sentences, layer=6):\n",
    "    \"\"\"\n",
    "    Extract activation of the final token at specified layer.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        sentences: List of strings\n",
    "        layer: Which layer to extract from (0-11 for GPT-2 small)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_sentences, d_model)\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Run model and cache activations\n",
    "        _, cache = model.run_with_cache(sentence)\n",
    "        \n",
    "        # Extract residual stream at specified layer\n",
    "        # Shape: [batch=1, seq_len, d_model=768]\n",
    "        layer_acts = cache[\"resid_post\", layer]\n",
    "        \n",
    "        # Get final token's activation\n",
    "        final_act = layer_acts[0, -1, :].cpu().numpy()\n",
    "        activations.append(final_act)\n",
    "    \n",
    "    return np.array(activations)\n",
    "\n",
    "# Test it\n",
    "test_acts = get_final_token_activation(model, [\"Test sentence\"], layer=6)\n",
    "print(f\"Activation shape: {test_acts.shape}\")  # Should be (1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for our dataset at layer 6\n",
    "print(\"Extracting activations... (this may take a minute)\")\n",
    "\n",
    "X_pos = get_final_token_activation(model, positive_sentences, layer=6)\n",
    "X_neg = get_final_token_activation(model, negative_sentences, layer=6)\n",
    "\n",
    "# Combine into single dataset\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.array([1]*len(positive_sentences) + [0]*len(negative_sentences))\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # (40, 768)\n",
    "print(f\"y shape: {y.shape}\")  # (40,)\n",
    "print(f\"Class balance: {np.sum(y)} positive, {len(y) - np.sum(y)} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Probe\n",
    "\n",
    "**What's happening:** Logistic regression learns a weight vector (direction in 768-dim space) that best separates positive from negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the probe\n",
    "probe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "probe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = probe.score(X_train, y_train)\n",
    "test_acc = probe.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n=== Probe Performance ===\")\n",
    "print(f\"Train accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test accuracy:  {test_acc:.2%}\")\n",
    "print(f\"Train/test gap: {(train_acc - test_acc):.2%}\")\n",
    "\n",
    "# Detailed test set performance\n",
    "y_pred = probe.predict(X_test)\n",
    "print(f\"\\n{classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "**Before moving on, think about:**\n",
    "1. Is your test accuracy >70%? (Success criterion)\n",
    "2. Is the train/test gap <15%? (Overfitting check)\n",
    "3. What does this tell you about sentiment being linearly accessible at layer 6?\n",
    "\n",
    "**Write your observations here:**\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Layer Comparison (2 hours)\n",
    "\n",
    "**Question:** Which layer has the most accessible sentiment information?\n",
    "\n",
    "**Hypothesis:** Later layers should be better (information gets refined as it flows through).\n",
    "\n",
    "**Why this matters:** For CoT faithfulness, understanding WHERE in the model information crystallizes is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple layers\n",
    "layers_to_test = [0, 2, 4, 6, 8, 10, 11]\n",
    "results = []\n",
    "\n",
    "print(\"Testing multiple layers... (this will take a few minutes)\\n\")\n",
    "\n",
    "for layer in layers_to_test:\n",
    "    # Extract activations at this layer\n",
    "    X_pos = get_final_token_activation(model, positive_sentences, layer=layer)\n",
    "    X_neg = get_final_token_activation(model, negative_sentences, layer=layer)\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.array([1]*len(positive_sentences) + [0]*len(negative_sentences))\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train probe\n",
    "    probe_layer = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    probe_layer.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = probe_layer.score(X_train, y_train)\n",
    "    test_acc = probe_layer.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'layer': layer,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'gap': train_acc - test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"Layer {layer:2d}: Train={train_acc:.2%}, Test={test_acc:.2%}, Gap={train_acc - test_acc:.2%}\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by layer\n",
    "ax1.plot(df_results['layer'], df_results['train_acc'], marker='o', label='Train', linewidth=2)\n",
    "ax1.plot(df_results['layer'], df_results['test_acc'], marker='s', label='Test', linewidth=2)\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Sentiment Detection Accuracy by Layer', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0.4, 1.0])\n",
    "\n",
    "# Plot 2: Train/test gap\n",
    "ax2.bar(df_results['layer'], df_results['gap'], color='coral', alpha=0.7)\n",
    "ax2.axhline(y=0.15, color='red', linestyle='--', label='15% threshold')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Train - Test Accuracy', fontsize=12)\n",
    "ax2.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sentiment_probe_layer_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest performing layer (test acc): Layer {df_results.loc[df_results['test_acc'].idxmax(), 'layer']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Questions\n",
    "\n",
    "**Look at your plots and answer:**\n",
    "\n",
    "1. **Does accuracy improve in later layers?** Why or why not?\n",
    "   - Your answer:\n",
    "\n",
    "2. **Which layer has the best test accuracy?** Is this surprising?\n",
    "   - Your answer:\n",
    "\n",
    "3. **Are any layers severely overfitting (gap >15%)?** What could cause this?\n",
    "   - Your answer:\n",
    "\n",
    "4. **Connection to CoT faithfulness:** If you were probing for \"faithful reasoning,\" would you focus on early, middle, or late layers? Why?\n",
    "   - Your answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Generalization Testing (2-3 hours)\n",
    "\n",
    "**Critical Question:** Does the probe learn actual sentiment, or spurious correlations?\n",
    "\n",
    "**This is THE KEY CONCEPT for your faithfulness research!**\n",
    "\n",
    "**Test 1:** Different writing style (but same sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different distribution: More formal/elaborate style\n",
    "different_positive = [\n",
    "    \"Absolutely delightful experience today.\",\n",
    "    \"Couldn't be happier with the outcome.\",\n",
    "    \"This has genuinely exceeded my expectations.\",\n",
    "    \"I find this remarkably satisfying.\",\n",
    "    \"The quality is truly exceptional.\",\n",
    "    \"I'm thoroughly impressed by this.\",\n",
    "    \"This represents outstanding value.\",\n",
    "    \"I'm genuinely pleased with this choice.\",\n",
    "    \"The experience was notably positive.\",\n",
    "    \"This is remarkably well executed.\",\n",
    "]\n",
    "\n",
    "different_negative = [\n",
    "    \"Utterly disappointing and frustrating.\",\n",
    "    \"Completely unsatisfactory results.\",\n",
    "    \"This failed to meet basic expectations.\",\n",
    "    \"I find this remarkably unsatisfying.\",\n",
    "    \"The quality is truly poor.\",\n",
    "    \"I'm thoroughly unimpressed by this.\",\n",
    "    \"This represents terrible value.\",\n",
    "    \"I'm genuinely displeased with this choice.\",\n",
    "    \"The experience was notably negative.\",\n",
    "    \"This is remarkably poorly executed.\",\n",
    "]\n",
    "\n",
    "print(f\"Different distribution: {len(different_positive)} positive, {len(different_negative)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the probe trained on layer 6 from Part 1\n",
    "# (Make sure you still have 'probe' from earlier)\n",
    "\n",
    "# Extract activations from different distribution\n",
    "X_diff_pos = get_final_token_activation(model, different_positive, layer=6)\n",
    "X_diff_neg = get_final_token_activation(model, different_negative, layer=6)\n",
    "X_diff = np.vstack([X_diff_pos, X_diff_neg])\n",
    "y_diff = np.array([1]*len(different_positive) + [0]*len(different_negative))\n",
    "\n",
    "# Evaluate on different distribution\n",
    "diff_acc = probe.score(X_diff, y_diff)\n",
    "y_diff_pred = probe.predict(X_diff)\n",
    "\n",
    "print(f\"\\n=== Generalization Test ===\")\n",
    "print(f\"Original test accuracy: {test_acc:.2%}\")\n",
    "print(f\"Different distribution accuracy: {diff_acc:.2%}\")\n",
    "print(f\"Accuracy drop: {(test_acc - diff_acc):.2%}\")\n",
    "print(f\"\\n{classification_report(y_diff, y_diff_pred, target_names=['Negative', 'Positive'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Analysis\n",
    "\n",
    "**Answer these carefully (they're crucial for understanding probes):**\n",
    "\n",
    "1. **Why might probe get 90% on test set but 60% on different distribution?**\n",
    "   - Your answer:\n",
    "\n",
    "2. **What spurious correlations might the probe have learned?** (Examples: sentence length, specific words, punctuation)\n",
    "   - Your answer:\n",
    "\n",
    "3. **How would you test if probe detects sentiment vs. sentence length?**\n",
    "   - Your answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Spurious Correlation Testing (Bonus, 2 hours)\n",
    "\n",
    "**Let's actually test for spurious correlations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hypothesis: Does probe use sentence length as a shortcut?\n",
    "\n",
    "# Create dataset where positive = SHORT, negative = LONG\n",
    "# (opposite of typical pattern)\n",
    "short_positive = [\n",
    "    \"Great!\",\n",
    "    \"Love it!\",\n",
    "    \"Perfect!\",\n",
    "    \"Amazing!\",\n",
    "    \"Excellent!\",\n",
    "]\n",
    "\n",
    "long_negative = [\n",
    "    \"This is absolutely terrible, disappointing, and completely unsatisfactory in every possible way.\",\n",
    "    \"I cannot express how utterly frustrated and unhappy I am with this poor quality product.\",\n",
    "    \"The experience was remarkably negative, frustrating, and failed to meet any of my expectations.\",\n",
    "    \"This represents a terrible value proposition with numerous flaws and significant shortcomings throughout.\",\n",
    "    \"I am thoroughly displeased and disappointed with every aspect of this regrettable purchase decision.\",\n",
    "]\n",
    "\n",
    "# Test if probe gets confused\n",
    "X_short_pos = get_final_token_activation(model, short_positive, layer=6)\n",
    "X_long_neg = get_final_token_activation(model, long_negative, layer=6)\n",
    "X_adversarial = np.vstack([X_short_pos, X_long_neg])\n",
    "y_adversarial = np.array([1]*len(short_positive) + [0]*len(long_negative))\n",
    "\n",
    "adv_acc = probe.score(X_adversarial, y_adversarial)\n",
    "y_adv_pred = probe.predict(X_adversarial)\n",
    "\n",
    "print(f\"\\n=== Adversarial Test (Length vs Sentiment) ===\")\n",
    "print(f\"Accuracy: {adv_acc:.2%}\")\n",
    "print(f\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_adversarial, y_adv_pred))\n",
    "print(f\"\\nIf probe relied on length: accuracy would be ~0%\")\n",
    "print(f\"If probe detects true sentiment: accuracy should be ~100%\")\n",
    "print(f\"\\nActual accuracy: {adv_acc:.2%} - What does this tell you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Reflection: Connecting to Faithfulness\n",
    "\n",
    "**You've now experienced the core challenges of probe-based detection.**\n",
    "\n",
    "**For your CoT faithfulness research (Week 4), you'll face:**\n",
    "\n",
    "1. **The generalization problem:** Just like sentiment probe might fail on different writing styles, faithfulness probe might fail when reasoning style changes\n",
    "\n",
    "2. **The spurious correlation problem:** Just like probe might use length instead of sentiment, faithfulness probe might use \"word 'therefore'\" instead of actual faithfulness\n",
    "\n",
    "3. **The layer selection problem:** You need to know WHERE in the model to probe (just like we tested multiple layers)\n",
    "\n",
    "4. **The position selection problem:** You need to know WHICH tokens to probe (conclusion tokens? all tokens?)\n",
    "\n",
    "**Your hypotheses (H1-H4) directly test these challenges!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Before moving to Day 5-6 (Advanced Techniques):**\n",
    "\n",
    "✅ Confirm you can:\n",
    "- Extract activations from any layer\n",
    "- Train a probe and evaluate it\n",
    "- Compare performance across layers\n",
    "- Test generalization on different distributions\n",
    "- Identify potential spurious correlations\n",
    "\n",
    "✅ Understand:\n",
    "- What probes detect (linearly accessible information)\n",
    "- When probes fail (non-linear, spurious correlations, distribution shift)\n",
    "- Why this matters for faithfulness detection\n",
    "\n",
    "**Day 5-6 Preview:** Token position analysis, cross-position probing, activation extraction from MLP/attention layers\n",
    "\n",
    "---\n",
    "\n",
    "**Save your work and take notes on what you learned!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
