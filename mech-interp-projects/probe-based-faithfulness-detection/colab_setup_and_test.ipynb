{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Setup for Probe Research\n",
    "\n",
    "**Purpose:** Set up Google Colab environment with GPU for mechanistic interpretability work.\n",
    "\n",
    "**Steps:**\n",
    "1. Enable GPU in Colab\n",
    "2. Install required packages\n",
    "3. Verify GPU access\n",
    "4. Test transformer_lens with GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è FIRST: Enable GPU in Colab\n",
    "\n",
    "**Before running any code:**\n",
    "1. Click **Runtime** ‚Üí **Change runtime type**\n",
    "2. Set **Hardware accelerator** to **GPU** (T4 or better)\n",
    "3. Click **Save**\n",
    "\n",
    "Then proceed with the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU Availability Check\n",
      "==================================================\n",
      "‚úÖ CUDA available: True\n",
      "‚úÖ CUDA version: 12.6\n",
      "‚úÖ GPU device: Tesla T4\n",
      "‚úÖ GPU memory: 15.83 GB\n",
      "\n",
      "üöÄ Ready for GPU-accelerated computations!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU Availability Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"\\nüöÄ Ready for GPU-accelerated computations!\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"‚ö†Ô∏è  Go to Runtime ‚Üí Change runtime type ‚Üí Set Hardware accelerator to GPU\")\n",
    "    print(\"Then restart this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Install Required Packages\n",
    "\n",
    "**Note:** Colab has PyTorch pre-installed. We just need to install:\n",
    "- transformer-lens (mechanistic interpretability)\n",
    "- Additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Package Installation Complete\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Install transformer-lens and dependencies\n",
    "!pip install transformer-lens -q\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Package Installation Complete\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages imported successfully!\n",
      "‚úÖ NumPy: 1.26.4\n",
      "‚úÖ PyTorch: 2.9.0+cu126\n",
      "‚úÖ TransformerLens ready!\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import transformer_lens as tl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ TransformerLens ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test GPU with Transformer Model\n",
    "\n",
    "**This will:**\n",
    "1. Load GPT-2 small on GPU\n",
    "2. Run inference\n",
    "3. Measure speed vs. CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e8bb3b245a486983969a45e12d2c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89bbe5ce3a846d9adec88d6b7141ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165429c587fb471498f95df77acdd32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05227fd7d20f4ba9b3b40d70bc506809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cca5a70a4d4042979225f2e6460153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56daf8ac1f3e4eac9cdb8fc2f5c8d449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e137d9844c12442e907461370326af21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "‚úÖ Model loaded on GPU: Tesla T4\n",
      "\n",
      "Model specs: 12 layers, 768 dimensions\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Loading GPT-2 small...\")\n",
    "model = tl.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(f\"‚úÖ Model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model on CPU (slower)\")\n",
    "\n",
    "print(f\"\\nModel specs: {model.cfg.n_layers} layers, {model.cfg.d_model} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running speed test...\n",
      "\n",
      "‚è±Ô∏è  Inference time: 34.34 ms\n",
      "\n",
      "Top predictions:\n",
      "  1. ' now' (prob: 4.75%)\n",
      "  2. ' the' (prob: 3.74%)\n",
      "  3. ' a' (prob: 3.55%)\n",
      "  4. ' home' (prob: 3.09%)\n",
      "  5. ' in' (prob: 2.70%)\n",
      "\n",
      "‚úÖ GPU inference working!\n"
     ]
    }
   ],
   "source": [
    "# Speed test\n",
    "test_prompt = \"The capital of France is\"\n",
    "\n",
    "print(\"Running speed test...\\n\")\n",
    "\n",
    "# Warm-up run\n",
    "_ = model.run_with_cache(test_prompt)\n",
    "\n",
    "# Timed run\n",
    "start = time.time()\n",
    "logits, cache = model.run_with_cache(test_prompt)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚è±Ô∏è  Inference time: {(end - start) * 1000:.2f} ms\")\n",
    "print(f\"\\nTop predictions:\")\n",
    "top_tokens = logits[0, -1].topk(5)\n",
    "for i in range(5):\n",
    "    token_id = top_tokens.indices[i]\n",
    "    token_str = model.tokenizer.decode(token_id)\n",
    "    prob = torch.softmax(logits[0, -1], dim=-1)[token_id].item()\n",
    "    print(f\"  {i+1}. '{token_str}' (prob: {prob:.2%})\")\n",
    "\n",
    "print(f\"\\n‚úÖ GPU inference working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Test Activation Extraction on GPU\n",
    "\n",
    "**This tests the core operation for probe research.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations from layer 6...\n",
      "\n",
      "‚úÖ 'I love this movie!...' ‚Üí activation shape: (768,)\n",
      "‚úÖ 'This is terrible....' ‚Üí activation shape: (768,)\n",
      "‚úÖ 'The weather is nice today....' ‚Üí activation shape: (768,)\n",
      "\n",
      "‚è±Ô∏è  Total time: 0.10 seconds\n",
      "üìä Combined activations shape: (3, 768)\n",
      "\n",
      "‚úÖ Activation extraction working on GPU!\n"
     ]
    }
   ],
   "source": [
    "def get_final_token_activation(model, sentence, layer=6):\n",
    "    \"\"\"\n",
    "    Extract activation of final token at specified layer.\n",
    "    Works on both CPU and GPU.\n",
    "    \"\"\"\n",
    "    _, cache = model.run_with_cache(sentence)\n",
    "    layer_acts = cache[\"resid_post\", layer]\n",
    "    final_act = layer_acts[0, -1, :].cpu().numpy()  # Move to CPU for numpy\n",
    "    return final_act\n",
    "\n",
    "# Test with multiple sentences\n",
    "test_sentences = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is terrible.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "print(\"Extracting activations from layer 6...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "activations = []\n",
    "for sent in test_sentences:\n",
    "    act = get_final_token_activation(model, sent, layer=6)\n",
    "    activations.append(act)\n",
    "    print(f\"‚úÖ '{sent[:30]}...' ‚Üí activation shape: {act.shape}\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "activations = np.array(activations)\n",
    "print(f\"\\n‚è±Ô∏è  Total time: {(end - start):.2f} seconds\")\n",
    "print(f\"üìä Combined activations shape: {activations.shape}\")\n",
    "print(f\"\\n‚úÖ Activation extraction working on GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Quick Probe Test on GPU\n",
    "\n",
    "**Let's verify the full pipeline works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for probe test...\n",
      "‚úÖ Dataset: 10 examples, 768 features\n",
      "\n",
      "=== Probe Performance ===\n",
      "Train accuracy: 100.00%\n",
      "Test accuracy:  100.00%\n",
      "\n",
      "üéâ Full pipeline working on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Small sentiment dataset\n",
    "positive = [\"I love this!\", \"Amazing work!\", \"Great job!\", \"Fantastic!\", \"Excellent!\"]\n",
    "negative = [\"I hate this.\", \"Terrible work.\", \"Poor job.\", \"Awful.\", \"Disappointing.\"]\n",
    "\n",
    "print(\"Extracting activations for probe test...\")\n",
    "\n",
    "# Extract activations\n",
    "X_pos = np.array([get_final_token_activation(model, s, layer=6) for s in positive])\n",
    "X_neg = np.array([get_final_token_activation(model, s, layer=6) for s in negative])\n",
    "\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.array([1]*len(positive) + [0]*len(negative))\n",
    "\n",
    "print(f\"‚úÖ Dataset: {X.shape[0]} examples, {X.shape[1]} features\")\n",
    "\n",
    "# Train probe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "probe = LogisticRegression(max_iter=1000)\n",
    "probe.fit(X_train, y_train)\n",
    "\n",
    "train_acc = probe.score(X_train, y_train)\n",
    "test_acc = probe.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n=== Probe Performance ===\")\n",
    "print(f\"Train accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test accuracy:  {test_acc:.2%}\")\n",
    "\n",
    "print(f\"\\nüéâ Full pipeline working on GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GPU Memory Management Tips\n",
    "\n",
    "**Important for longer experiments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU Memory Usage\n",
      "==================================================\n",
      "Allocated: 0.87 GB\n",
      "Reserved:  1.02 GB\n",
      "Total:     15.83 GB\n",
      "Free:      14.80 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"GPU Memory Usage\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Reserved:  {reserved:.2f} GB\")\n",
    "    print(f\"Total:     {total:.2f} GB\")\n",
    "    print(f\"Free:      {total - reserved:.2f} GB\")\n",
    "    \n",
    "    # Clear cache if needed\n",
    "    # torch.cuda.empty_cache()\n",
    "    # print(\"\\n‚úÖ CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "‚úÖ **Setup complete!** You can now:\n",
    "\n",
    "1. **Upload your probe notebooks** to Colab and run them with GPU\n",
    "2. **Process larger datasets** faster\n",
    "3. **Test multiple layers** simultaneously\n",
    "4. **Run Week 4 experiments** with better performance\n",
    "\n",
    "**To save your work:**\n",
    "- File ‚Üí Download ‚Üí Download .ipynb\n",
    "- Or: File ‚Üí Save a copy in Drive (recommended)\n",
    "\n",
    "**Colab tips:**\n",
    "- Sessions last ~12 hours with activity\n",
    "- Save frequently (Ctrl+S)\n",
    "- Download important results\n",
    "- Free tier: ~15-20 hours GPU/week\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to start probe research with GPU acceleration!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mech_interp_env_py311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
