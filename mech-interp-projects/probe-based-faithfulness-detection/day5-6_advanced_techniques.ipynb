{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5-6: Advanced Techniques\n",
    "\n",
    "**Goal:** Expand probe toolkit with position analysis, attention head probing, and component-specific extraction\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Compare different token positions for probe accuracy\n",
    "2. Extract and probe attention head outputs\n",
    "3. Extract and probe MLP layer outputs\n",
    "4. Build a reusable ProbeToolkit class\n",
    "\n",
    "**Timeline:** 6-8 hours\n",
    "\n",
    "**Why This Matters for Faithfulness Detection:**\n",
    "- H2 predicts information is concentrated at conclusion tokens (\"therefore\", \"so\")\n",
    "- We need to systematically compare positions to test this\n",
    "- Different model components may encode different aspects of faithfulness\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import transformer_lens as tl\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small\n",
    "model = tl.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "print(f\"Model loaded: {model.cfg.n_layers} layers, {model.cfg.d_model} dimensions\")\n",
    "print(f\"Attention heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"Head dimension: {model.cfg.d_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse dataset from Day 3-4\n",
    "positive_sentences = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is amazing and wonderful!\",\n",
    "    \"Great job, fantastic work!\",\n",
    "    \"Absolutely brilliant performance!\",\n",
    "    \"I'm so happy with the results!\",\n",
    "    \"This exceeded all my expectations!\",\n",
    "    \"Wonderful experience, highly recommend!\",\n",
    "    \"Best decision I ever made!\",\n",
    "    \"This is perfect in every way!\",\n",
    "    \"I'm thrilled with how this turned out!\",\n",
    "    \"Outstanding quality and service!\",\n",
    "    \"This brings me so much joy!\",\n",
    "    \"Incredible work, truly impressive!\",\n",
    "    \"I'm delighted with this purchase!\",\n",
    "    \"This is exactly what I needed!\",\n",
    "    \"Five stars, absolutely love it!\",\n",
    "    \"This made my day so much better!\",\n",
    "    \"I'm grateful for this opportunity!\",\n",
    "    \"This is surprisingly excellent!\",\n",
    "    \"I'm really pleased with the outcome!\",\n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"I hate this movie.\",\n",
    "    \"This is terrible and awful.\",\n",
    "    \"Poor job, disappointing work.\",\n",
    "    \"Absolutely dreadful performance.\",\n",
    "    \"I'm so unhappy with the results.\",\n",
    "    \"This failed all my expectations.\",\n",
    "    \"Terrible experience, avoid this.\",\n",
    "    \"Worst decision I ever made.\",\n",
    "    \"This is flawed in every way.\",\n",
    "    \"I'm upset with how this turned out.\",\n",
    "    \"Poor quality and bad service.\",\n",
    "    \"This brings me so much frustration.\",\n",
    "    \"Mediocre work, not impressive.\",\n",
    "    \"I'm disappointed with this purchase.\",\n",
    "    \"This is not what I needed.\",\n",
    "    \"One star, completely hate it.\",\n",
    "    \"This ruined my day completely.\",\n",
    "    \"I regret this opportunity.\",\n",
    "    \"This is surprisingly horrible.\",\n",
    "    \"I'm really displeased with the outcome.\",\n",
    "]\n",
    "\n",
    "print(f\"Dataset: {len(positive_sentences)} positive, {len(negative_sentences)} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Token Position Analysis (2-3 hours)\n",
    "\n",
    "**Question:** Which token position contains the most sentiment information?\n",
    "\n",
    "**Options to explore:**\n",
    "- **Last token:** Common choice, contains accumulated information\n",
    "- **First token:** Beginning of sentence\n",
    "- **Mean pooling:** Average across all tokens\n",
    "- **Max pooling:** Maximum activation across positions\n",
    "- **Specific positions:** e.g., position 0, 1, 2, etc.\n",
    "\n",
    "**For CoT faithfulness:** This maps directly to H2 - whether information is concentrated at conclusion markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get ALL Token Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_token_activations(model, sentences, layer=6):\n",
    "    \"\"\"\n",
    "    Get activations for ALL tokens in each sentence.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        sentences: List of strings\n",
    "        layer: Which layer to extract from\n",
    "    \n",
    "    Returns:\n",
    "        List of numpy arrays, each of shape (seq_len, d_model)\n",
    "        (Note: different sentences may have different seq_len)\n",
    "    \"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        _, cache = model.run_with_cache(sentence)\n",
    "        # Shape: [1, seq_len, d_model]\n",
    "        acts = cache[\"resid_post\", layer][0, :, :].cpu().numpy()\n",
    "        all_activations.append(acts)\n",
    "    \n",
    "    return all_activations\n",
    "\n",
    "# Test it\n",
    "test_acts = get_all_token_activations(model, [\"Hello world!\"], layer=6)\n",
    "print(f\"Single sentence activations shape: {test_acts[0].shape}\")\n",
    "\n",
    "# Show token breakdown\n",
    "tokens = model.to_tokens(\"Hello world!\")\n",
    "print(f\"Tokens: {[model.tokenizer.decode([t]) for t in tokens[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extract Features by Position Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(activations, method='last'):\n",
    "    \"\"\"\n",
    "    Extract single feature vector from variable-length activations.\n",
    "    \n",
    "    Args:\n",
    "        activations: List of numpy arrays, each (seq_len, d_model)\n",
    "        method: 'last', 'first', 'mean', 'max'\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_samples, d_model)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for acts in activations:\n",
    "        if method == 'last':\n",
    "            features.append(acts[-1])\n",
    "        elif method == 'first':\n",
    "            features.append(acts[0])\n",
    "        elif method == 'mean':\n",
    "            features.append(acts.mean(axis=0))\n",
    "        elif method == 'max':\n",
    "            features.append(acts.max(axis=0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Demo\n",
    "test_acts = get_all_token_activations(model, [\"Test sentence here\"], layer=6)\n",
    "for method in ['last', 'first', 'mean', 'max']:\n",
    "    feat = extract_features(test_acts, method)\n",
    "    print(f\"{method}: shape {feat.shape}, mean={feat.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compare Position Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_probe(X_pos, X_neg, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Helper function to train probe and return results.\n",
    "    \"\"\"\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.array([1]*len(X_pos) + [0]*len(X_neg))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    probe = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "    probe.fit(X_train, y_train)\n",
    "    \n",
    "    return {\n",
    "        'probe': probe,\n",
    "        'train_acc': probe.score(X_train, y_train),\n",
    "        'test_acc': probe.score(X_test, y_test),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all activations once (for efficiency)\n",
    "print(\"Extracting all token activations...\")\n",
    "pos_all_acts = get_all_token_activations(model, positive_sentences, layer=6)\n",
    "neg_all_acts = get_all_token_activations(model, negative_sentences, layer=6)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different position methods\n",
    "methods = ['last', 'first', 'mean', 'max']\n",
    "position_results = []\n",
    "\n",
    "print(\"\\n=== Position Method Comparison ===\\n\")\n",
    "\n",
    "for method in methods:\n",
    "    X_pos = extract_features(pos_all_acts, method)\n",
    "    X_neg = extract_features(neg_all_acts, method)\n",
    "    \n",
    "    results = train_and_evaluate_probe(X_pos, X_neg)\n",
    "    \n",
    "    position_results.append({\n",
    "        'method': method,\n",
    "        'train_acc': results['train_acc'],\n",
    "        'test_acc': results['test_acc'],\n",
    "        'gap': results['train_acc'] - results['test_acc']\n",
    "    })\n",
    "    \n",
    "    print(f\"{method:6s}: Train={results['train_acc']:.2%}, Test={results['test_acc']:.2%}\")\n",
    "\n",
    "df_position = pd.DataFrame(position_results)\n",
    "print(f\"\\nBest method: {df_position.loc[df_position['test_acc'].idxmax(), 'method']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_position['train_acc'], width, label='Train', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, df_position['test_acc'], width, label='Test', color='coral')\n",
    "\n",
    "ax.set_xlabel('Position Method', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Sentiment Detection: Position Method Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.set_ylim([0.4, 1.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.0%}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('position_method_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Position-by-Position Analysis\n",
    "\n",
    "**Let's go deeper:** Which specific token positions carry the most information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_at_position(activations, position):\n",
    "    \"\"\"\n",
    "    Extract activation at specific position.\n",
    "    \n",
    "    Args:\n",
    "        activations: List of numpy arrays\n",
    "        position: int (positive) or negative (from end)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_valid_samples, d_model)\n",
    "        List of valid indices (samples that had enough tokens)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, acts in enumerate(activations):\n",
    "        seq_len = acts.shape[0]\n",
    "        \n",
    "        # Handle negative indices (from end)\n",
    "        if position < 0:\n",
    "            actual_pos = seq_len + position\n",
    "        else:\n",
    "            actual_pos = position\n",
    "        \n",
    "        # Check if position is valid for this sequence\n",
    "        if 0 <= actual_pos < seq_len:\n",
    "            features.append(acts[actual_pos])\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    return np.array(features), valid_indices\n",
    "\n",
    "# Test\n",
    "test_feat, test_idx = extract_at_position(pos_all_acts, position=0)\n",
    "print(f\"Position 0: {len(test_feat)} valid samples\")\n",
    "test_feat, test_idx = extract_at_position(pos_all_acts, position=-1)\n",
    "print(f\"Position -1 (last): {len(test_feat)} valid samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple positions\n",
    "positions_to_test = [0, 1, 2, 3, -3, -2, -1]  # First few and last few\n",
    "position_specific_results = []\n",
    "\n",
    "print(\"\\n=== Position-Specific Analysis ===\\n\")\n",
    "\n",
    "for pos in positions_to_test:\n",
    "    X_pos_feat, pos_idx = extract_at_position(pos_all_acts, pos)\n",
    "    X_neg_feat, neg_idx = extract_at_position(neg_all_acts, pos)\n",
    "    \n",
    "    # Only proceed if we have enough samples\n",
    "    if len(X_pos_feat) >= 5 and len(X_neg_feat) >= 5:\n",
    "        # Need balanced dataset - take minimum\n",
    "        min_samples = min(len(X_pos_feat), len(X_neg_feat))\n",
    "        X_pos_feat = X_pos_feat[:min_samples]\n",
    "        X_neg_feat = X_neg_feat[:min_samples]\n",
    "        \n",
    "        results = train_and_evaluate_probe(X_pos_feat, X_neg_feat)\n",
    "        \n",
    "        position_specific_results.append({\n",
    "            'position': pos,\n",
    "            'n_samples': min_samples * 2,\n",
    "            'train_acc': results['train_acc'],\n",
    "            'test_acc': results['test_acc'],\n",
    "        })\n",
    "        \n",
    "        pos_label = f\"{pos}\" if pos >= 0 else f\"last{pos+1}\" if pos != -1 else \"last\"\n",
    "        print(f\"Position {pos:3d}: n={min_samples*2:2d}, Train={results['train_acc']:.2%}, Test={results['test_acc']:.2%}\")\n",
    "    else:\n",
    "        print(f\"Position {pos:3d}: Insufficient samples (pos={len(X_pos_feat)}, neg={len(X_neg_feat)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position-specific results\n",
    "if position_specific_results:\n",
    "    df_pos_specific = pd.DataFrame(position_specific_results)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    positions = [str(p) for p in df_pos_specific['position']]\n",
    "    x = np.arange(len(positions))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, df_pos_specific['train_acc'], width, label='Train', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, df_pos_specific['test_acc'], width, label='Test', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Token Position', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Sentiment Detection by Token Position', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(positions)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.3, 1.05])\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('position_specific_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Analysis Reflection\n",
    "\n",
    "**Answer these questions:**\n",
    "\n",
    "1. **Which position(s) work best?** Is this what you expected?\n",
    "   - Your answer:\n",
    "\n",
    "2. **Why might the last token contain more sentiment information?**\n",
    "   - Your answer:\n",
    "\n",
    "3. **For CoT faithfulness detection, which positions might be most informative?** (Think about where conclusions appear)\n",
    "   - Your answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Attention Head Probing (2-3 hours)\n",
    "\n",
    "**Question:** Do specific attention heads specialize in sentiment?\n",
    "\n",
    "**Background:**\n",
    "- GPT-2 small has 12 layers × 12 heads = 144 attention heads\n",
    "- Each head produces a d_head=64 dimensional output\n",
    "- Different heads may specialize in different tasks\n",
    "\n",
    "**Why this matters:** Understanding which components encode information helps us target probes effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_head_output(model, sentences, layer=6, head=0):\n",
    "    \"\"\"\n",
    "    Extract output from a specific attention head.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        sentences: List of strings\n",
    "        layer: Which layer (0-11 for GPT-2 small)\n",
    "        head: Which head (0-11 for GPT-2 small)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_sentences, d_head)\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        _, cache = model.run_with_cache(sentence)\n",
    "        \n",
    "        # Get attention head result\n",
    "        # Shape: [batch, pos, n_heads, d_head]\n",
    "        # This is the output of the attention head BEFORE being projected by W_O\n",
    "        head_output = cache[f\"blocks.{layer}.attn.hook_result\"]\n",
    "        \n",
    "        # Get last token's activation for the specified head\n",
    "        # Shape: [d_head] = [64]\n",
    "        head_act = head_output[0, -1, head, :].cpu().numpy()\n",
    "        activations.append(head_act)\n",
    "    \n",
    "    return np.array(activations)\n",
    "\n",
    "# Test\n",
    "test_head = get_attention_head_output(model, [\"Test sentence\"], layer=6, head=0)\n",
    "print(f\"Attention head output shape: {test_head.shape}\")  # Should be (1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all heads at a single layer\n",
    "layer_to_scan = 6\n",
    "head_results = []\n",
    "\n",
    "print(f\"\\n=== Scanning All Heads at Layer {layer_to_scan} ===\\n\")\n",
    "\n",
    "for head in range(model.cfg.n_heads):  # 12 heads\n",
    "    X_pos = get_attention_head_output(model, positive_sentences, layer=layer_to_scan, head=head)\n",
    "    X_neg = get_attention_head_output(model, negative_sentences, layer=layer_to_scan, head=head)\n",
    "    \n",
    "    results = train_and_evaluate_probe(X_pos, X_neg)\n",
    "    \n",
    "    head_results.append({\n",
    "        'layer': layer_to_scan,\n",
    "        'head': head,\n",
    "        'train_acc': results['train_acc'],\n",
    "        'test_acc': results['test_acc'],\n",
    "    })\n",
    "    \n",
    "    print(f\"Head {head:2d}: Train={results['train_acc']:.2%}, Test={results['test_acc']:.2%}\")\n",
    "\n",
    "df_heads = pd.DataFrame(head_results)\n",
    "best_head = df_heads.loc[df_heads['test_acc'].idxmax()]\n",
    "print(f\"\\nBest head: {best_head['head']:.0f} with test accuracy {best_head['test_acc']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize head comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(model.cfg.n_heads)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_heads['train_acc'], width, label='Train', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, df_heads['test_acc'], width, label='Test', color='coral', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Attention Head', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title(f'Sentiment Detection by Attention Head (Layer {layer_to_scan})', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "ax.set_ylim([0.3, 1.05])\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'attention_head_comparison_layer{layer_to_scan}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Layer Head Heatmap\n",
    "\n",
    "**Let's see which heads across ALL layers carry sentiment information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all layers and heads (this will take a few minutes)\n",
    "layers_to_scan = [0, 3, 6, 9, 11]  # Sample of layers\n",
    "all_head_results = []\n",
    "\n",
    "print(\"Scanning heads across layers... (this may take 2-3 minutes)\\n\")\n",
    "\n",
    "for layer in layers_to_scan:\n",
    "    print(f\"Layer {layer}...\", end=\" \")\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        X_pos = get_attention_head_output(model, positive_sentences, layer=layer, head=head)\n",
    "        X_neg = get_attention_head_output(model, negative_sentences, layer=layer, head=head)\n",
    "        \n",
    "        results = train_and_evaluate_probe(X_pos, X_neg)\n",
    "        \n",
    "        all_head_results.append({\n",
    "            'layer': layer,\n",
    "            'head': head,\n",
    "            'test_acc': results['test_acc'],\n",
    "        })\n",
    "    print(\"done\")\n",
    "\n",
    "print(\"\\nComplete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap\n",
    "df_all_heads = pd.DataFrame(all_head_results)\n",
    "pivot_heads = df_all_heads.pivot(index='layer', columns='head', values='test_acc')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(pivot_heads, annot=True, fmt='.0%', cmap='RdYlGn', \n",
    "            center=0.5, vmin=0.3, vmax=1.0, ax=ax,\n",
    "            cbar_kws={'label': 'Test Accuracy'})\n",
    "\n",
    "ax.set_xlabel('Attention Head', fontsize=12)\n",
    "ax.set_ylabel('Layer', fontsize=12)\n",
    "ax.set_title('Sentiment Detection: Layer × Head Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_head_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find best heads\n",
    "top_heads = df_all_heads.nlargest(5, 'test_acc')\n",
    "print(\"\\nTop 5 sentiment-detecting heads:\")\n",
    "for _, row in top_heads.iterrows():\n",
    "    print(f\"  Layer {row['layer']:.0f}, Head {row['head']:.0f}: {row['test_acc']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head Reflection\n",
    "\n",
    "1. **Are some heads notably better than others?** What does this suggest?\n",
    "   - Your answer:\n",
    "\n",
    "2. **Do later-layer heads tend to perform better?** Why might this be?\n",
    "   - Your answer:\n",
    "\n",
    "3. **How does head dimensionality (64) compare to residual stream (768)?** What are the tradeoffs?\n",
    "   - Your answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MLP Layer Probing (1-2 hours)\n",
    "\n",
    "**Question:** Does the MLP output contain different information than the residual stream?\n",
    "\n",
    "**Background:**\n",
    "- Each transformer block: Attention → Add to residual → MLP → Add to residual\n",
    "- MLP is thought to store factual knowledge\n",
    "- Residual stream accumulates information from all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_output(model, sentences, layer=6):\n",
    "    \"\"\"\n",
    "    Extract MLP layer output.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        sentences: List of strings\n",
    "        layer: Which layer\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_sentences, d_model)\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        _, cache = model.run_with_cache(sentence)\n",
    "        \n",
    "        # MLP output (before adding to residual)\n",
    "        # Shape: [batch, pos, d_model]\n",
    "        mlp_out = cache[f\"blocks.{layer}.hook_mlp_out\"]\n",
    "        \n",
    "        # Get last token\n",
    "        act = mlp_out[0, -1, :].cpu().numpy()\n",
    "        activations.append(act)\n",
    "    \n",
    "    return np.array(activations)\n",
    "\n",
    "# Test\n",
    "test_mlp = get_mlp_output(model, [\"Test sentence\"], layer=6)\n",
    "print(f\"MLP output shape: {test_mlp.shape}\")  # Should be (1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Residual vs MLP across layers\n",
    "layers_to_test = [0, 3, 6, 9, 11]\n",
    "component_results = []\n",
    "\n",
    "print(\"\\n=== Residual vs MLP Comparison ===\\n\")\n",
    "\n",
    "for layer in layers_to_test:\n",
    "    # Residual stream\n",
    "    X_pos_resid = get_all_token_activations(model, positive_sentences, layer=layer)\n",
    "    X_neg_resid = get_all_token_activations(model, negative_sentences, layer=layer)\n",
    "    X_pos_resid = extract_features(X_pos_resid, 'last')\n",
    "    X_neg_resid = extract_features(X_neg_resid, 'last')\n",
    "    resid_results = train_and_evaluate_probe(X_pos_resid, X_neg_resid)\n",
    "    \n",
    "    # MLP output\n",
    "    X_pos_mlp = get_mlp_output(model, positive_sentences, layer=layer)\n",
    "    X_neg_mlp = get_mlp_output(model, negative_sentences, layer=layer)\n",
    "    mlp_results = train_and_evaluate_probe(X_pos_mlp, X_neg_mlp)\n",
    "    \n",
    "    component_results.append({\n",
    "        'layer': layer,\n",
    "        'component': 'residual',\n",
    "        'test_acc': resid_results['test_acc']\n",
    "    })\n",
    "    component_results.append({\n",
    "        'layer': layer,\n",
    "        'component': 'mlp',\n",
    "        'test_acc': mlp_results['test_acc']\n",
    "    })\n",
    "    \n",
    "    print(f\"Layer {layer:2d}: Residual={resid_results['test_acc']:.2%}, MLP={mlp_results['test_acc']:.2%}\")\n",
    "\n",
    "df_components = pd.DataFrame(component_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df_resid = df_components[df_components['component'] == 'residual']\n",
    "df_mlp = df_components[df_components['component'] == 'mlp']\n",
    "\n",
    "ax.plot(df_resid['layer'], df_resid['test_acc'], marker='o', linewidth=2, \n",
    "        label='Residual Stream', color='steelblue', markersize=8)\n",
    "ax.plot(df_mlp['layer'], df_mlp['test_acc'], marker='s', linewidth=2, \n",
    "        label='MLP Output', color='coral', markersize=8)\n",
    "\n",
    "ax.set_xlabel('Layer', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Sentiment Detection: Residual Stream vs MLP', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.4, 1.0])\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_vs_mlp.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Analysis Reflection\n",
    "\n",
    "1. **Does MLP or residual stream perform better?** Why might this be?\n",
    "   - Your answer:\n",
    "\n",
    "2. **What information might MLP encode that residual doesn't (or vice versa)?**\n",
    "   - Your answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building the ProbeToolkit Class (1-2 hours)\n",
    "\n",
    "**Goal:** Create a reusable class that encapsulates everything we've learned\n",
    "\n",
    "**This toolkit will be essential for Week 4's faithfulness detection experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbeToolkit:\n",
    "    \"\"\"\n",
    "    Reusable toolkit for probe experiments.\n",
    "    \n",
    "    Supports:\n",
    "    - Multiple layers\n",
    "    - Multiple positions (last, first, mean, max, specific index)\n",
    "    - Multiple components (residual, mlp, attention heads)\n",
    "    - Systematic comparison across configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.n_layers = model.cfg.n_layers\n",
    "        self.n_heads = model.cfg.n_heads\n",
    "        self.d_model = model.cfg.d_model\n",
    "        self.d_head = model.cfg.d_head\n",
    "    \n",
    "    def extract_activations(self, sentences, layer, position='last', component='residual', head=None):\n",
    "        \"\"\"\n",
    "        Flexible activation extraction.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of strings\n",
    "            layer: Layer number (0 to n_layers-1)\n",
    "            position: 'last', 'first', 'mean', 'max', or int for specific position\n",
    "            component: 'residual', 'mlp', or 'attention'\n",
    "            head: Head number (required if component='attention')\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (n_sentences, d)\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            _, cache = self.model.run_with_cache(sentence)\n",
    "            \n",
    "            # Get raw activations based on component\n",
    "            if component == 'residual':\n",
    "                # Shape: [1, seq_len, d_model]\n",
    "                raw_acts = cache[\"resid_post\", layer][0, :, :].cpu().numpy()\n",
    "            elif component == 'mlp':\n",
    "                # Shape: [1, seq_len, d_model]\n",
    "                raw_acts = cache[f\"blocks.{layer}.hook_mlp_out\"][0, :, :].cpu().numpy()\n",
    "            elif component == 'attention':\n",
    "                if head is None:\n",
    "                    raise ValueError(\"Must specify head for attention component\")\n",
    "                # Shape: [1, seq_len, n_heads, d_head]\n",
    "                raw_acts = cache[f\"blocks.{layer}.attn.hook_result\"][0, :, head, :].cpu().numpy()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown component: {component}\")\n",
    "            \n",
    "            # Extract based on position\n",
    "            if position == 'last':\n",
    "                act = raw_acts[-1]\n",
    "            elif position == 'first':\n",
    "                act = raw_acts[0]\n",
    "            elif position == 'mean':\n",
    "                act = raw_acts.mean(axis=0)\n",
    "            elif position == 'max':\n",
    "                act = raw_acts.max(axis=0)\n",
    "            elif isinstance(position, int):\n",
    "                if position < 0:\n",
    "                    act = raw_acts[position]  # Negative indexing\n",
    "                elif position < len(raw_acts):\n",
    "                    act = raw_acts[position]\n",
    "                else:\n",
    "                    # Position out of bounds - use last\n",
    "                    act = raw_acts[-1]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown position: {position}\")\n",
    "            \n",
    "            activations.append(act)\n",
    "        \n",
    "        return np.array(activations)\n",
    "    \n",
    "    def train_probe(self, X_pos, X_neg, test_size=0.3, random_state=42):\n",
    "        \"\"\"\n",
    "        Train probe with proper train/test split.\n",
    "        \n",
    "        Returns:\n",
    "            dict with 'probe', 'train_acc', 'test_acc', 'X_test', 'y_test'\n",
    "        \"\"\"\n",
    "        X = np.vstack([X_pos, X_neg])\n",
    "        y = np.array([1]*len(X_pos) + [0]*len(X_neg))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        probe = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "        probe.fit(X_train, y_train)\n",
    "        \n",
    "        return {\n",
    "            'probe': probe,\n",
    "            'train_acc': probe.score(X_train, y_train),\n",
    "            'test_acc': probe.score(X_test, y_test),\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'predictions': probe.predict(X_test),\n",
    "            'probabilities': probe.predict_proba(X_test)\n",
    "        }\n",
    "    \n",
    "    def systematic_comparison(self, pos_sentences, neg_sentences,\n",
    "                             layers=None, positions=['last'], \n",
    "                             components=['residual']):\n",
    "        \"\"\"\n",
    "        Compare probes across multiple configurations.\n",
    "        \n",
    "        Args:\n",
    "            pos_sentences: Positive class sentences\n",
    "            neg_sentences: Negative class sentences\n",
    "            layers: List of layers to test (default: sample across model)\n",
    "            positions: List of positions to test\n",
    "            components: List of components to test\n",
    "        \n",
    "        Returns:\n",
    "            pandas DataFrame with results\n",
    "        \"\"\"\n",
    "        if layers is None:\n",
    "            # Default: sample layers across the model\n",
    "            layers = [0, self.n_layers//4, self.n_layers//2, \n",
    "                     3*self.n_layers//4, self.n_layers-1]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            for position in positions:\n",
    "                for component in components:\n",
    "                    print(f\"Testing: layer={layer}, pos={position}, comp={component}\")\n",
    "                    \n",
    "                    try:\n",
    "                        X_pos = self.extract_activations(\n",
    "                            pos_sentences, layer, position, component\n",
    "                        )\n",
    "                        X_neg = self.extract_activations(\n",
    "                            neg_sentences, layer, position, component\n",
    "                        )\n",
    "                        \n",
    "                        probe_results = self.train_probe(X_pos, X_neg)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'layer': layer,\n",
    "                            'position': str(position),\n",
    "                            'component': component,\n",
    "                            'train_acc': probe_results['train_acc'],\n",
    "                            'test_acc': probe_results['test_acc'],\n",
    "                            'gap': probe_results['train_acc'] - probe_results['test_acc']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def test_generalization(self, probe, new_pos, new_neg, layer, \n",
    "                           position='last', component='residual'):\n",
    "        \"\"\"\n",
    "        Test trained probe on new distribution.\n",
    "        \"\"\"\n",
    "        X_new_pos = self.extract_activations(new_pos, layer, position, component)\n",
    "        X_new_neg = self.extract_activations(new_neg, layer, position, component)\n",
    "        X_new = np.vstack([X_new_pos, X_new_neg])\n",
    "        y_new = np.array([1]*len(new_pos) + [0]*len(new_neg))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': probe.score(X_new, y_new),\n",
    "            'predictions': probe.predict(X_new),\n",
    "            'true_labels': y_new\n",
    "        }\n",
    "    \n",
    "    def get_probe_direction(self, probe):\n",
    "        \"\"\"\n",
    "        Extract the learned direction from a trained probe.\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (d,) - the direction separating classes\n",
    "        \"\"\"\n",
    "        return probe.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the toolkit\n",
    "toolkit = ProbeToolkit(model)\n",
    "\n",
    "print(f\"Model info: {toolkit.n_layers} layers, {toolkit.d_model} dimensions\")\n",
    "print(f\"Attention: {toolkit.n_heads} heads × {toolkit.d_head} dimensions each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run systematic comparison with the toolkit\n",
    "print(\"\\n=== Systematic Comparison with ProbeToolkit ===\\n\")\n",
    "\n",
    "results_df = toolkit.systematic_comparison(\n",
    "    positive_sentences, \n",
    "    negative_sentences,\n",
    "    layers=[0, 3, 6, 9, 11],\n",
    "    positions=['last', 'mean'],\n",
    "    components=['residual', 'mlp']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize systematic comparison results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Layer comparison by component\n",
    "for component in results_df['component'].unique():\n",
    "    df_comp = results_df[(results_df['component'] == component) & \n",
    "                         (results_df['position'] == 'last')]\n",
    "    axes[0].plot(df_comp['layer'], df_comp['test_acc'], \n",
    "                marker='o', linewidth=2, label=component, markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('Layer', fontsize=12)\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('Component Comparison (position=last)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.4, 1.0])\n",
    "\n",
    "# Plot 2: Heatmap of all configurations\n",
    "# Create combined config column for heatmap\n",
    "results_df['config'] = results_df['position'] + '_' + results_df['component']\n",
    "pivot = results_df.pivot(index='layer', columns='config', values='test_acc')\n",
    "\n",
    "sns.heatmap(pivot, annot=True, fmt='.0%', cmap='RdYlGn', \n",
    "            center=0.5, ax=axes[1], cbar_kws={'label': 'Test Accuracy'})\n",
    "axes[1].set_title('All Configurations Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('systematic_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Findings\n",
    "\n",
    "**Document your findings from Day 5-6:**\n",
    "\n",
    "### Position Analysis\n",
    "- Best position method: \n",
    "- Best specific position(s): \n",
    "- Key insight: \n",
    "\n",
    "### Attention Head Analysis\n",
    "- Best performing heads: \n",
    "- Layer pattern: \n",
    "- Key insight: \n",
    "\n",
    "### Component Analysis\n",
    "- Residual vs MLP: \n",
    "- Key insight: \n",
    "\n",
    "### For CoT Faithfulness Detection\n",
    "Based on what I learned, my predictions for faithfulness detection:\n",
    "- Best layers to probe: \n",
    "- Best positions: \n",
    "- Components to prioritize: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Before moving to Week 2 (Reasoning Models):**\n",
    "\n",
    "✅ Confirm you can:\n",
    "- Extract activations at any position (last, first, mean, specific index)\n",
    "- Extract attention head outputs\n",
    "- Extract MLP outputs\n",
    "- Use the ProbeToolkit for systematic comparisons\n",
    "\n",
    "✅ Understand:\n",
    "- How position affects probe accuracy\n",
    "- Which attention heads carry task-relevant information\n",
    "- Difference between residual stream and MLP representations\n",
    "\n",
    "**Week 2 Preview:** \n",
    "- Setting up reasoning models (Qwen via nnsight or Gemini API)\n",
    "- Understanding Chain-of-Thought structure\n",
    "- Building CoT-specific analysis tools\n",
    "\n",
    "---\n",
    "\n",
    "**Save your work and the ProbeToolkit class - you'll use it extensively in Week 4!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
